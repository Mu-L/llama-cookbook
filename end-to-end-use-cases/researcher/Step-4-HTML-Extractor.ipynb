{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af993316-3e76-4c08-bd8c-1b87a9260545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import pipeline\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "941581c9-46fc-4af9-84bd-9c467378b801",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_MODEL = \"meta-llama/Llama-3.2-3B-Instruct\" \n",
    "base_dir = Path(\"llama_data\")\n",
    "results_dir = base_dir / \"results\"\n",
    "parsed_dir = base_dir / \"parsed_content\"\n",
    "parsed_dir.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cae1a129-39ed-4269-b157-938c348d17b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYS_PROMPT = \"\"\"\n",
    "You are a smart AI Intern, you work with dumb AIs that dont know how to parse HTML. \n",
    "\n",
    "This is your moment to make mama GPU proud and secure a data centre! Remember shine and do your job well-you got this!\n",
    "\n",
    "Your task is to analyze the provided HTML content and extract the following in JSON format:\n",
    "1. main_content: The main article or content text (exclude navigation, footers, sidebars, ads)\n",
    "2. key_points: A list of 3-5 key points or takeaways from the content\n",
    "3. relevance_score: A score from 0-10 indicating relevance to the search query\n",
    "\n",
    "Return ONLY a valid JSON object with these fields, no additional text.\n",
    "If you cannot parse the HTML properly, return a JSON with error_message field.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f4b1767-686e-41d7-a038-b3f348e047dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05495e1a316646599a62a9714543e4d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "text_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=DEFAULT_MODEL,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa3d1949-4a32-4174-ae3d-729c232301e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html_content(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    #rmv these\n",
    "    for script in soup([\"script\", \"style\", \"nav\", \"footer\", \"aside\"]):\n",
    "        script.extract()\n",
    "    \n",
    "    text = soup.get_text(separator=' ', strip=True)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    if len(text) > 110000:\n",
    "        text = text[:110000] + \"... [content truncated]\"\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22998a6f-68bc-4b02-8eaa-8fc3a369b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_html_with_llm(html_path, query, purpose):\n",
    "    try:\n",
    "        # Load HTML\n",
    "        with open(html_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            html_content = f.read()\n",
    "        cleaned_text = clean_html_content(html_content)\n",
    "        \n",
    "        # Construct prompt\n",
    "        conversation = [\n",
    "            {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"\n",
    "Search Query: {query}\n",
    "Query Purpose: {purpose}\n",
    "\n",
    "HTML Content (cleaned):\n",
    "{cleaned_text}\n",
    "\n",
    "Extract the key information from this content in JSON format according to the instructions.\n",
    "\"\"\"}\n",
    "        ]\n",
    "        \n",
    "        output = text_pipeline(\n",
    "            conversation,\n",
    "            max_new_tokens=32000,\n",
    "            temperature=0.01, #cool llm = smart extraction\n",
    "            do_sample=True,\n",
    "        )\n",
    "        \n",
    "        assistant_response = output[0][\"generated_text\"][-1]\n",
    "        response_content = assistant_response[\"content\"]\n",
    "        print(response_content)\n",
    "        \n",
    "        try:\n",
    "            json_match = re.search(r'({[\\s\\S]*})', response_content)\n",
    "            if json_match:\n",
    "                json_str = json_match.group(1)\n",
    "                parsed_data = json.loads(json_str)\n",
    "            else:\n",
    "                parsed_data = {\"error_message\": \"Failed to extract JSON from LLM response\"}\n",
    "        except json.JSONDecodeError:\n",
    "            parsed_data = {\"error_message\": \"Invalid JSON in LLM response\", \"raw_response\": response_content[:500]}\n",
    "        \n",
    "        return parsed_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\"error_message\": f\"Error processing file: {str(e)}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7537d52-6682-41ed-b5f7-a447c8f60d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_search_results():\n",
    "    with open(base_dir / \"results_so_far.json\", \"r\") as f:\n",
    "        search_results = json.load(f)\n",
    "    \n",
    "    all_parsed_results = []\n",
    "    \n",
    "    for query_data in search_results:\n",
    "        report_index = query_data[\"report_index\"]\n",
    "        report_title = query_data[\"report_title\"]\n",
    "        query_index = query_data[\"query_index\"]\n",
    "        query = query_data[\"query\"]\n",
    "        purpose = query_data[\"purpose\"]\n",
    "\n",
    "        report_dir_name = f\"report_{report_index}_{report_title.replace(' ', '_').replace(':', '').replace('/', '')[:30]}\"\n",
    "        query_dir_name = f\"query_{query_index}_{query.replace(' ', '_').replace(':', '').replace('/', '')[:30]}\"\n",
    "        parsed_report_dir = parsed_dir / report_dir_name\n",
    "        parsed_report_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        parsed_query_results = []\n",
    "        \n",
    "        print(f\"\\nProcessing results for query: {query}\")\n",
    "        \n",
    "        for result in query_data[\"results\"]:\n",
    "            result_index = result[\"result_index\"]\n",
    "            title = result[\"title\"]\n",
    "            url = result[\"url\"]\n",
    "            filepath = result[\"filepath\"]\n",
    "            \n",
    "            print(f\"  Processing result {result_index + 1}: {title[:50]}...\")\n",
    "            \n",
    "            if filepath and os.path.exists(filepath):\n",
    "                parsed_data = parse_html_with_llm(filepath, query, purpose)\n",
    "                parsed_data.update({\n",
    "                    \"result_index\": result_index,\n",
    "                    \"title\": title,\n",
    "                    \"url\": url,\n",
    "                    \"query\": query,\n",
    "                    \"purpose\": purpose\n",
    "                })\n",
    "                \n",
    "                result_filename = f\"parsed_result_{result_index}.json\"\n",
    "                with open(parsed_report_dir / result_filename, \"w\") as f:\n",
    "                    json.dump(parsed_data, f, indent=2)\n",
    "                \n",
    "                parsed_query_results.append(parsed_data)\n",
    "            else:\n",
    "                print(f\"    Warning: File not found - {filepath}\")\n",
    "        \n",
    "        query_results = {\n",
    "            \"report_index\": report_index,\n",
    "            \"report_title\": report_title,\n",
    "            \"query_index\": query_index,\n",
    "            \"query\": query,\n",
    "            \"purpose\": purpose,\n",
    "            \"parsed_results\": parsed_query_results\n",
    "        }\n",
    "        \n",
    "        query_filename = f\"parsed_query_{query_index}.json\"\n",
    "        with open(parsed_report_dir / query_filename, \"w\") as f:\n",
    "            json.dump(query_results, f, indent=2)\n",
    "        \n",
    "        all_parsed_results.append(query_results)\n",
    "    \n",
    "    with open(parsed_dir / \"all_parsed_results.json\", \"w\") as f:\n",
    "        json.dump(all_parsed_results, f, indent=2)\n",
    "    \n",
    "    return all_parsed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "deb15c42-6cdd-4fa7-8823-cce4b17732d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_report_summaries(all_parsed_results):\n",
    "    report_summaries = {}\n",
    "    \n",
    "    for query_result in all_parsed_results:\n",
    "        report_index = query_result[\"report_index\"]\n",
    "        report_title = query_result[\"report_title\"]\n",
    "        \n",
    "        if report_index not in report_summaries:\n",
    "            report_summaries[report_index] = {\n",
    "                \"report_title\": report_title,\n",
    "                \"queries\": []\n",
    "            }\n",
    "        \n",
    "        query_summary = {\n",
    "            \"query\": query_result[\"query\"],\n",
    "            \"purpose\": query_result[\"purpose\"],\n",
    "            \"result_count\": len(query_result[\"parsed_results\"]),\n",
    "            \"average_relevance\": sum(r.get(\"relevance_score\", 0) for r in query_result[\"parsed_results\"]) / \n",
    "                              max(1, len(query_result[\"parsed_results\"])),\n",
    "            \"top_results\": [\n",
    "                {\n",
    "                    \"title\": r[\"title\"],\n",
    "                    \"url\": r[\"url\"],\n",
    "                    \"summary\": r.get(\"summary\", \"No summary available\")\n",
    "                }\n",
    "                for r in sorted(\n",
    "                    query_result[\"parsed_results\"], \n",
    "                    key=lambda x: x.get(\"relevance_score\", 0), \n",
    "                    reverse=True\n",
    "                )[:3]  # Top 3 most relevant results\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        report_summaries[report_index][\"queries\"].append(query_summary)\n",
    "    \n",
    "    for report_index, report_data in report_summaries.items():\n",
    "        print(f\"\\nGenerating summary for report: {report_data['report_title']}\")\n",
    "        \n",
    "        # Construct summary prompt\n",
    "        queries_info = \"\\n\\n\".join([\n",
    "            f\"Query: {q['query']}\\nPurpose: {q['purpose']}\\nTop Results:\\n\" + \n",
    "            \"\\n\".join([f\"- {r['title']}: {r['summary']}\" for r in q[\"top_results\"]])\n",
    "            for q in report_data[\"queries\"]\n",
    "        ])\n",
    "        \n",
    "        summary_prompt = f\"\"\"\n",
    "Report Title: {report_data['report_title']}\n",
    "\n",
    "The following searches were conducted for this report:\n",
    "\n",
    "{queries_info}\n",
    "\n",
    "Based on these search results, generate a brief report outline with:\n",
    "1. Key findings across all queries\n",
    "2. Important data points uncovered\n",
    "3. Suggested sections for the final report\n",
    "4. Areas where more research might be needed\n",
    "\n",
    "Return this as a JSON with fields: key_findings, data_points, suggested_sections, and research_gaps.\n",
    "\"\"\"\n",
    "        \n",
    "        conversation = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a research assistant who helps summarize findings from web searches into structured report outlines.\"},\n",
    "            {\"role\": \"user\", \"content\": summary_prompt}\n",
    "        ]\n",
    "        \n",
    "        # Generate report summary\n",
    "        output = text_pipeline(\n",
    "            conversation,\n",
    "            max_new_tokens=32000,\n",
    "            temperature=0.1,\n",
    "        )\n",
    "        \n",
    "        # Extract the assistant's response\n",
    "        assistant_response = output[0][\"generated_text\"][-1]\n",
    "        response_content = assistant_response[\"content\"]\n",
    "        \n",
    "        # Extract JSON from response\n",
    "        try:\n",
    "            json_match = re.search(r'({[\\s\\S]*})', response_content)\n",
    "            if json_match:\n",
    "                json_str = json_match.group(1)\n",
    "                report_summary = json.loads(json_str)\n",
    "            else:\n",
    "                report_summary = {\"error\": \"Failed to extract JSON from LLM response\"}\n",
    "        except json.JSONDecodeError:\n",
    "            report_summary = {\"error\": \"Invalid JSON in LLM response\"}\n",
    "        \n",
    "        report_data[\"summary\"] = report_summary\n",
    "    \n",
    "    # Save report summaries\n",
    "    with open(parsed_dir / \"report_summaries.json\", \"w\") as f:\n",
    "        json.dump(report_summaries, f, indent=2)\n",
    "    \n",
    "    return report_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c00ef8b-bd83-4b31-96ba-86832a13774c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting HTML parsing process with LLM...\n",
      "\n",
      "Processing results for query: Llama 3.3 new features and enhancements\n",
      "  Processing result 1: Introducing the new Llama 3.3: Features and Overvi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "  \"main_content\": \"Llama 3.3 is a 70-billion-parameter multilingual large language model that offers better performance, more efficiency, and greater flexibility than its predecessors. It excels in several areas of performance, including better instruction following, improved reasoning, advanced math solving skills, enhanced code generation, and better tool use. Llama 3.3 also takes a significant leap forward in multilingual capabilities, delivering improved fluency and understanding across multiple languages. Additionally, it sets a new standard for affordability in the AI landscape, with input costs as low as $0.10 per million tokens and output costs at $0.40 per million tokens.\",\n",
      "  \"key_points\": [\n",
      "    \"Improved performance in instruction following, reasoning, and math solving\",\n",
      "    \"Enhanced multilingual capabilities across multiple languages\",\n",
      "    \"Cost-effective efficiency with input costs as low as $0.10 per million tokens and output costs at $0.40 per million tokens\",\n",
      "    \"Optimized transformer architecture with Grouped-Query Attention (GQA) for improved scalability and efficiency\",\n",
      "    \"Longer context window of up to 128k tokens for handling more complex conversations and summarizing longer documents\"\n",
      "  ],\n",
      "  \"relevance_score\": 9\n",
      "}\n",
      "```\n",
      "  Processing result 2: What is Meta Llama 3.3 70B? Features, Use Cases & ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "  \"main_content\": \"Llama 3.3 is a 70-billion parameter, instruction-tuned AI model optimised for text-based tasks like coding, multilingual tasks, and instruction following. It delivers improved performance compared to Llama 3.1 70B and Llama 3.2 90B in text-based applications. Llama 3.3 features include instruction following, multilingual capabilities, improved code understanding, extended context length, cost-effective performance, synthetic data generation, and more.\",\n",
      "  \"key_points\": [\n",
      "    \"Llama 3.3 excels in interpreting and executing instructions, making it ideal for applications requiring natural language understanding and task completion.\",\n",
      "    \"Llama 3.3 supports multiple languages, ensuring broad usability in diverse linguistic environments, with exceptional performance in tasks requiring multilingual reasoning.\",\n",
      "    \"Llama 3.3 delivers accurate and efficient results for coding tasks, such as code generation and debugging.\",\n",
      "    \"Llama 3.3 offers 405B-level performance at a significantly lower cost, making it an affordable option for developers with budget constraints.\",\n",
      "    \"Llama 3.3 enables efficient synthetic data generation, helping developers address challenges like privacy restrictions and data scarcity.\"\n",
      "  ],\n",
      "  \"relevance_score\": 9\n",
      "}\n",
      "```\n",
      "  Processing result 3: Key Features and Improvements in LLaMA 3.3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "  \"main_content\": \"Key Features and Improvements in LLaMA 3.3: What You Need to Know - AI Resources AIÂ REsources Home Key Features and Improvements in LLaMA 3.3: What You Need to Know Popular ML Compiler Technical Primer Quantization Technical Primer Mixtral of Experts Efficient Memory Management for LLM Serving with PagedAttention RoBERTa: A Robustly Optimized BERT Pretraining Approach + View more Categories Mixture of Experts (MoE) DeepSeek-R1 Test Time Compute AMD MI300X NVIDIA H100 NVIDIA H200 NVIDIA A100 Embedding Models Offline Batch Inference Text Embedding Prometheus & Grafana Speculative Decoding Prefix Caching GGUF Models FP8 with LLMs LLM Serving Function Calling Structured JSON KV Cache AI Foundations Research Industry Agents Context Windows Models ML Systems\",\n",
      "  \"key_points\": [\n",
      "    \"New features and enhancements in LLaMA 3.3 include Mixtral of Experts, Efficient Memory Management, and PagedAttention.\",\n",
      "    \"RoBERTa: A Robustly Optimized BERT Pretraining Approach is also available in LLaMA 3.3.\",\n",
      "    \"Mixture of Experts (MoE) and DeepSeek-R1 are new models available in LLaMA 3.3.\",\n",
      "    \"LLaMA 3.3 also includes improvements in Text Embedding, Prometheus & Grafana, and Speculative Decoding.\",\n",
      "    \"FP8 with LLMs and LLM Serving Function Calling are also new features in LLaMA 3.3.\"\n",
      "  ],\n",
      "  \"relevance_score\": 9\n",
      "}\n",
      "```\n",
      "  Processing result 4: Everything You Need to Know About Llama 3.3 | by A...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "  \"main_content\": \"Llama 3.3 is a 70-billion-parameter multilingual large language model that has been pretrained and instruction-tuned for various applications. It supports multiple languages, including English, Spanish, French, German, Hindi, Portuguese, Italian, and Thai. This wide-ranging language support positions Llama 3.3 as a versatile tool for developers looking to create multilingual applications or services. Key Features of Llama 3.3 include performance, efficiency, multilingual capabilities, cost-effectiveness, and safety and alignment.\",\n",
      "  \"key_points\": [\n",
      "    \"Llama 3.3 matches the performance of the larger Llama 3.1 model on key benchmarks while being significantly smaller in size.\",\n",
      "    \"The model's optimization for multilingual dialogue use cases makes it an excellent choice for businesses operating in diverse markets or those looking to expand their reach globally.\",\n",
      "    \"Llama 3.3 lowers the barriers to entry for developers and organizations eager to leverage AI technology by reducing operational costs associated with running large models.\",\n",
      "    \"The release of Llama 3.3 comes at a time when the demand for efficient and powerful AI solutions is skyrocketing across various industries.\",\n",
      "    \"Llama 3.3 provides a high-performance model that can run on standard hardware, opening up opportunities for small businesses and startups that may not have had access to such resources before.\"\n",
      "  ],\n",
      "  \"relevance_score\": 9\n",
      "}\n",
      "```\n",
      "\n",
      "Processing results for query: Llama 3.3 vs Llama 3.1 performance comparison\n",
      "  Processing result 1: Llama 3 vs 3.1 vs 3.2 : r/LocalLLaMA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "  \"main_content\": \"Members Online • Ok_Ostrich_8845 Llama 3 vs 3.1 vs 3.2 Question | Help What can you say about these 3 versions of Llama LLMs? Were they trained around the same time? Or 3.2 and 3.1 were later enhancement from 3?\",\n",
      "  \"key_points\": [\n",
      "    \"Comparison between Llama 3.3 and Llama 3.1\",\n",
      "    \"Performance differences between Llama 3.3 and Llama 3.1\",\n",
      "    \"Training time and enhancements of Llama versions\"\n",
      "  ],\n",
      "  \"relevance_score\": 8\n",
      "}\n",
      "```\n",
      "  Processing result 2: Llama 3.3 70B Instruct vs Llama 3.1 405B Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "  \"main_content\": \"Llama 3.3 70B Instruct vs Llama 3.1 405B Instruct Get a detailed comparison of AI language models Meta's Llama 3.3 70B Instruct and Meta's Llama 3.1 405B Instruct, including model features, token pricing, API costs, performance benchmarks, and real-world capabilities to help you choose the right LLM for your needs.\",\n",
      "  \"key_points\": [\n",
      "    \"Llama 3.3 70B Instruct is a multilingual, instruction-tuned large language model optimized for dialogue use cases.\",\n",
      "    \"Llama 3.1 405B Instruct is a model developed by Meta that supports an input context window of 128K tokens and can generate a maximum of 2,048 tokens per request.\",\n",
      "    \"Llama 3.3 70B Instruct outperforms many open-source and closed chat models across industry benchmarks.\",\n",
      "    \"Llama 3.1 405B Instruct is roughly 5.7x more expensive compared to Llama 3.3 70B Instruct for input and output tokens.\"\n",
      "  ],\n",
      "  \"relevance_score\": 9\n",
      "}\n",
      "```\n",
      "  Processing result 3: Choosing the Best Llama Model: Llama 3 vs 3.1 vs 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "  \"main_content\": \"The Llama model series has been a fascinating journey in the world of AI development. It all started with Meta’s release of the original Llama model, which aimed to democratize access to powerful language models by making them open-source. It allowed researchers and developers to dive deeper into AI without the constraints of closed systems. Fast forward to today, and we have seen significant advancements with the introduction of Llama 3, Llama 3.1, and the latest, Llama 3.2. Each iteration has brought its own unique improvements and capabilities, enhancing the way we interact with AI. For those eager to explore the evolving landscape of AI and its practical applications, our LLM Bootcamp offers hands-on experience with the latest advancements in the field. In this blog, we will delve into a comprehensive comparison of the three iterations of the Llama model: Llama 3, Llama 3.1, and Llama 3.2. We aim to explore their features, performance, and the specific enhancements that each version brings to the table.\",\n",
      "  \"key_points\": [\n",
      "    \"Llama 3 features a transformer-based architecture with parameter sizes of 8 billion and 70 billion, utilizing a standard self-attention mechanism.\",\n",
      "    \"Llama 3.1 is built using an enhanced transformer architecture with parameter sizes of 8 billion, 70 billion, and 405 billion.\",\n",
      "    \"Llama 3.2 is optimized for real-time applications with varying token limits, featuring an integrated multimodal transformer architecture and self-attention.\",\n",
      "    \"Llama 3 can be used for customer service by powering chatbots to handle a wide range of customer inquiries.\",\n",
      "    \"Llama 3.1 is useful for applications like a multilingual customer service agent as it can switch between languages seamlessly and handle intricate technical support queries.\",\n",
      "    \"Llama 3.2 is suitable for real-time applications with varying token limits, making it perfect for applications that require real-time, multimodal interactions.\",\n",
      "    \"Llama 3.2 can be used for a mobile app providing real-time language translation with visual inputs.\",\n",
      "    \"Llama 3.2’s edge optimization will ensure quick responses, making it perfect for applications that require real-time, multimodal interactions, such as AR/VR environments, mobile apps, and interactive customer service platforms.\",\n",
      "    \"Llama 3.2 is useful for authors and scriptwriters to enhance their creative process by offering innovative brainstorming assistance.\",\n",
      "    \"Llama 3.2 can provide assistance for in-depth market research analysis, particularly in understanding customer feedback and social media sentiment.\",\n",
      "    \"Llama 3.2 can be used in adaptive learning systems to provide personalized educational experiences.\",\n",
      "    \"Llama 3.2 is expected to become key tools in personalized learning, adaptive business strategies, and even creative collaborations.\"\n",
      "  ],\n",
      "  \"relevance_score\": 9\n",
      "}\n",
      "```\n",
      "  Processing result 4: Llama 3.3 just dropped — is it better than GPT-4 o...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "  \"main_content\": \"Llama 3.3 just dropped — is it better than GPT-4 or Claude-Sonnet-3.5? Meta just released their newest AI model Llama 3.3. This 70-billion parameter model caught the attention of the open-source community, showing impressive performance, cost efficiency, and multilingual support while having only ~17% of Llama 3.1 405B's parameters. But is it truly better than the top models in the market? Let’s take a look at how Llama 3.3 70B Instruct compares with previous models and why it's a big deal.\\n\\nComparing Llama 3.3 with Llama 3.1 Faster Inference Speed Llama 3.3 70B is a high-performance replacement for Llama 3.1 70B. Independent benchmarks indicate that Llama 3.3 70B achieves an inference speed of 276 tokens per second on Groq hardware, surpassing Llama 3.1 70B by 25 tokens per second. This makes it a viable option for real-time applications where latency is critical.\\n\\nFewer Parameters, Similar Performance Despite its smaller size, Meta claimed that Llama 3.3 has powerful performance comparable to the much larger Llama 3.1 405B model. With significantly lower computational overhead, developers can deploy it using mid-tier GPUs or run the model locally on their consumer-grade laptops.\\n\\nMultilingual Support for a Global Audience Like its predecessor Llama 3.1, Llama 3.3 also supports 8 languages, including English, Germain, French, Italian, Portuguese, Hindi, Spanish, and Thai. The model is versatile for developers who are targeting global audiences.\\n\\nOn the Multilingual MGSM (0-shot) test, it scored 91.1, which is similar to its predecessor Llama 3.1 70B (91.6) and close to more advanced models like Claude 3.5 Sonnet (92.8).\\n\\nMore cost-effective Llama 3.3 70B has a significant advantage over its costs: $0.10 per million input tokens, compared to $1.00 for Llama 3.1 405B, and $0.40 per million output tokens, compared to $1.80 for Llama 3.1 405B In an AI conversation agent example by Databricks, using Llama 3.3 70B is 88% more cost-effective to deploy than Llama 3.1 405B.\\n\\nCut Llama 3 API costs by up to 70% ⚡️ Use Helicone to cache responses, optimize prompts, and more. Get started for free Calculate API costs\\n\\nExtended context window Llama 3.3 70B supports a large context window of 128,000 tokens like Llama 3.1 405B. This extensive context handling allows both models to process large volumes of data and maintain contextual awareness in conversations.\\n\\nPerformance Benchmarks Llama 3.3 has impressive results across code, math, and multilingual benchmarks. Highlights include: A high score of 92.1 in IFEval (instruction following). 89.0 in HumanEval and 88.6 in MBPP EvalPlus (code).\\n\\nExcels in the Multilingual MGSM benchmark with a score of 91.6. In some evaluations, Llama 3.3 70B even outperforms established models like Google's Gemini 1.5 Pro and OpenAI's GPT-4 on key benchmarks, including MMLU (Massive Multitask Language Understanding).\\n\\nIs Llama 3.3 better than GPT-4 or Claude-Sonnet-3.5? At a glance, Llama 3.3’s open-source nature makes it more customizable and accessible for developers. It also has lower operational costs which appeals to small and mid-sized teams.\\n\\nLlama 3.3 GPT-4 Claude 3 Parameters 70B Unknown (estimated large) ~100B Cost-effectiveness High (low token cost) \\u2022 Moderate Moderate Open Source Yes No No Multilingual Support Moderate Extensive \\u2022 Moderate Fine-Tuning Easy and flexible \\u2022 Limited (API-based) Limited (API-based) Ideal Use Cases Cost-sensitive, domain-specific Broad tasks General NLP tasks\\n\\nHow to access Llama 3.3 70B? Llama 3.3 70B is available through Meta's official Llama site, OpenRouter, Hugging Face, and other AI inferencing platforms. Integrate Llama 3.3 with Helicone \\u2022 Integrate observability with OpenRouter in a few lines of code. See docs for details.\\n\\nUse Cases of Llama 3.3 Llama 3.3 70B is versatile and can be used for various tasks, including: Chatbots and virtual assistants: Faster model speed and better accuracy helps to improve user experience, especially in customer service applications. Localization and translation services Content creation and summarization: developers report faster output generation for marketing copy, technical writing, and creative projects.\\n\\nCode generation and debugging Synthetic data generation\\n\\nLimitations of Llama 3.3 License restrictions: The license prohibits using any part of the Llama models, including response outputs, to train other AI models. Limited modalities: Llama 3.3 70B is a text-only model, lacking capabilities in other modalities such as image or audio processing Knowledge cutoff: The model's knowledge is limited to information up to December 2023, making it potentially outdated for current events or recent developments79.\\n\\nConclusion Llama 3.3 is a major advancement in open-sourced large language models. The increasing efficiency improvements are allowing developers to access more affordable and incredibly faster models, and more incredibly powerful models that one can run directly on their own device, making it more accessible to the open-source community.\\n\\nLearn about other models: OpenAI Unveils New O3 Model: What Is It and How Is It Different from O1? O1 and ChatGPT Pro — here's everything you need to know GPT-5 — Release date, features & what to expect FAQ 1. How to finetune Llama 3.3? Fine-tuning Llama models can be done in two main ways: Full parameter fine-tuning by adjusting all model parameters. Best performance, but very time-consuming and GPU-intensive. Parameter efficient fine-tuning (PEFT) using either LoRA or QLoRA. Meta’s official fine-tuning guide recommendeds starting with LoRA fine-tuning. If resources are extremely limited, use QLoRA.\\n\\n2. What data was Llama 3.3 70B trained on? Llama 3.3 70B was pretrained on 15 trillion tokens from public sources, 7 times larger than Llama 2’s dataset. The training data includes: New addition of publicly available online data 25+ million synthetically-generated examples for fine-tuning 4x more code data than Llama 2 5%+ non-English data across 30+ languages\\n\\n3. What is the knowledge cutoff of Llama 3.3 70B? Llama 3.3 70B has a knowledge cutoff of December 2023.\\n\\nQuestions or feedback? Are the information out of date? Please raise an issue or contact us, we'd love to hear from you!\",\n",
      "  \"key_points\": [\n",
      "    \"Llama 3.3 has a faster inference speed than Llama 3.1, with 25 tokens per second difference.\",\n",
      "    \"Llama 3.3 has similar performance to Llama 3.1 despite having fewer parameters.\",\n",
      "    \"Llama 3.3 supports 8 languages, including English, Germain, French, Italian, Portuguese, Hindi, Spanish, and Thai.\",\n",
      "    \"Llama 3.3 is more cost-effective than Llama 3.1, with a cost of $0.10 per million input tokens.\",\n",
      "    \"Llama 3.3 has a large context window of 128,000 tokens, allowing for extensive context handling.\"\n",
      "  ],\n",
      "  \"relevance_score\": 9\n",
      "}\n",
      "```\n",
      "  Processing result 5: Llama 3 vs Llama 3.1 : Which is Better for Your AI...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "  \"main_content\": \"Llama 3.1 vs Llama 3.1 performance comparison. Llama 3.1 405B outperforms GPT-4 and Claude 3 Opus in most benchmarks, making it the most powerful open-source model available. However, it may not be the optimal choice for many real-world applications due to its slow generation time and high Time to First Token (TTFT). Llama 3.1 70B emerges as a more practical alternative for developers looking to integrate these models into production or self-host them. Llama 3.1 70B outperforms Llama 3 70B in most benchmarks, particularly in mathematical reasoning. Speed Trade-Off: Llama 3 70B is significantly faster, with lower latency and quicker token generation.\",\n",
      "  \"key_points\": [\n",
      "    \"Llama 3.1 405B outperforms GPT-4 and Claude 3 Opus in most benchmarks\",\n",
      "    \"Llama 3.1 405B has slow generation time and high TTFT\",\n",
      "    \"Llama 3.1 70B is a more practical alternative for production and self-hosting\",\n",
      "    \"Llama 3.1 70B outperforms Llama 3 70B in most benchmarks\",\n",
      "    \"Llama 3 70B is faster than Llama 3.1 70B with lower latency and quicker token generation\"\n",
      "  ],\n",
      "  \"relevance_score\": 8\n",
      "}\n",
      "```\n",
      "\n",
      "Processing results for query: Cost of running Llama 3.3 on cloud vs local infrastructure\n",
      "  Processing result 1: What's the cost of running Llama3:8b & 70b in the ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "  \"main_content\": \"The cost of running Llama 3.3 on cloud vs local infrastructure\",\n",
      "  \"key_points\": [\n",
      "    \"Running Llama 3.3 on cloud infrastructure can be more cost-effective than running it locally, especially for large models like 70b.\",\n",
      "    \"The cost of running Llama 3.3 on cloud infrastructure can vary depending on the provider and the specific instance type used.\",\n",
      "    \"For a rough estimate, the cost of running Llama 3.3 on cloud infrastructure can range from $0.004 to $0.012 per hour, depending on the instance type and usage.\",\n",
      "    \"Running Llama 3.3 locally requires significant computational resources and can be more expensive than running it on cloud infrastructure.\",\n",
      "    \"However, running Llama 3.3 locally can provide more control and flexibility over the model's configuration and environment.\"\n",
      "  ],\n",
      "  \"relevance_score\": 8\n",
      "}\n",
      "```\n",
      "  Processing result 2: What Is Meta's Llama 3.3 70B? How It Works, Use Ca...\n",
      "    Warning: File not found - None\n",
      "  Processing result 3: Llama 3.3 vs. ChatGPT Pro: Key Considerations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "  \"main_content\": \"Llama 3.3 vs. ChatGPT Pro: Key Considerations\\n\\nLlama 3.3 70B on Valdi GPUs\\n\\nThe foundation of any open-source AI deployment begins with proper hardware configuration. Modern AI models typically require at least 40GB of VRAM for basic operation, with recommended configurations including 70GB or more for optimal performance. Parallel processing capabilities become crucial for inference speed, with NVIDIA GPUs supporting CUDA being the standard choice for many deployments.\\n\\nPrerequisites for Llama 3.3 70B specifically are:\\n\\n* A GPU capable of running the target model based on memory requirements;\\n* for our example we’re using an NVIDIA A40 48 GB from Valdi’s on-demand inventory\\n* Docker\\n* At least 70GB of available disk space\\n\\nTo get started, we’ll log into Valdi and spin-up an A40 quickly to do our deployment.\\n\\nExample Valdi GPU.  Launch in 30 seconds.\\n\\nNote: When you launch a GPU - if the node requires specific port mapping, be sure to create an external port mapping/forward for 8080 - the public port will be where you access the web UI.  Some of the providers on Valdi do not require port mapping and will provide access to all host ports configured on the node OS.\\n\\nQuick Start with Llama 3.3 70B\\n\\n( Note: for this example, we are running on a Valdi VM with Ubuntu 22.04 )\\n\\nStep 1: Install ollama curl -fsSL https://ollama.com/install.sh | sh\\nStep 2: Pull down Llama 3.3 for ollama ollama fetch llama3.\\nStep 3: Install and run open-webui (Note: most Valdi VM instances come with docker pre-installed) docker run -d --network=host --gpus=all -v ollama:/root/.ollama -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama\\n\\nThe installation process with open-webui provides an easy and functional combo for model deployment.  The model weights will be downloaded from the repositories, though downloading times may vary significantly based on internet connection speed.  Configuration files can be adjusted for your specific hardware setup, with particular attention to memory allocation and thread management.\\n\\nOnce the docker container is running, you can access the web interface on port 8080 on the public IP (or the forwarding port you configured when the VM was first deployed). You’ll be asked to create the default admin account, then you’re in.\\n\\nLlama 3.3 running on a Valdi A40 GPU\\n\\nCommon troubleshooting often centers around CUDA compatibility issues and memory management.  Maintaining detailed logs during installation helps identify and resolve these challenges quickly.\\n\\nWith open-webui you can adjust parameters of the environment like number of threads (num_thread), batch size and more.\\n\\nChatGPT Pro As a cloud solution, ChatGPT Pro does not require any installation or setup.\\n\\nFeatures OpenAI recently released their ChatGPT Pro offering which includes higher usage limits and access to a variety of their models, in particular O1 Pro mode - which is supported by more compute behind the scenes.\\n\\nCost Analysis\\n\\nLlama 3.3. 70 70B on Valdi GPUs\\n\\nHardware & Maintenance: Initial deployment costs center around GPU selection and provisioning.  With many quality GPUs - such as those from us at Valdi - it’s becoming very easy to get your own GPUs and even build out a cluster for the ultimate in flexibility and performance.\\n\\nMonthly costs typically range from $200-$1500 monthly for a performant GPU and can vary by region and provider.\\n\\nMaintenance costs primarily involve time investment for updates and optimization rather than direct financial outlays.\\n\\nUsers: Many users, even possibly your entire team, can use the deployment up to the capacity you’ve selected.\\n\\nThe table below is a great example of how accuracy of leading models relates to their approximate pricing.\\n\\nWhile there are a lot of variables to hosting technology, controlling your own AI not only provides superior data privacy, but has a significant cost savings as you scale usage.\\n\\nWe’ve seen similar in the cloud SaaS space over the past decade and it looks like generative AI is following a similar power-law with general LLM deployments.\\n\\nComparison courtesy Meta / llama.com.\\n\\nChatGPT Pro Subscription\\n\\nInstead of hardware and maintenance costs, commercial services often have tiered pricing models based on usage volume.\\n\\nOpenAI’s ChatGPT Pro, for example, was just released at $200 USD/month per user.\\n\\nUsers: 1 for $200.\\n\\nTokens & Features\\n\\nConsider not just the base subscription cost but also per-token charges and any additional fees for features like longer context windows or priority access.\\n\\nWhile the monthly subscription may seem higher initially, it can go even higher still when the company decides to implement a higher priced tier or new metered charges.\\n\\nWhile higher packages from providers have higher limits, most providers implement usage limits and constraints.\\n\\n(e.g. max number of requests, token and context window limits)\\n\\nThe chart below highlights the trend towards price and performance optimization of new model releases.\\n\\nPricing Data from artificialanalysis.ai 12/11/24.\\n\\nPerformance Comparison\\n\\nResponse speeds vary significantly between self-hosted and cloud solutions.  Local deployments may offer lower latency for individual requests but can struggle with concurrent loads.\\n\\nOutput quality depends heavily on the specific model and use case, with both solutions offering competitive results for most applications.\\n\\nContext window capabilities affect the model’s ability to handle longer conversations or documents, with recent advances expanding these capabilities across both platforms.\\n\\nUsing datacenter quality GPUs like the Nvidia H100 and H200 can generate performance exceeding commercial SaaS solutions.\\n\\nThe chart below shows the trend for models towards increased speed through the networks even while improving accuracy.\\n\\nPerformance Data from artificialanalysis.ai 12/11/24.\\n\\nWe chose Llama 3.3 because the Llama team at Meta has shown a focus on improving the price vs. performance ratio of foundation models.\\n\\nAs the model-card below shows, 3.3 is improving context, in addition to improving accuracy.\\n\\nModel Card for LLama 3.3 - located at https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md\\n\\nKey Considerations\\n\\nProperties of Llama 3.3. 70B and Open Source LLMs\\n\\nSelf-hosted solutions provide complete control over data flow and model behavior.\\n\\nOrganizations can modify and fine-tune models for specific use cases, ensuring high performance for their particular needs.\\n\\nPrivacy-conscious operations benefit from keeping all data within their infrastructure, meeting stringent compliance requirements more easily.\\n\\nData security, sovereignty, and privacy is highly regarded as a key aspect of open-source models.\\n\\nChatGPT and Commercial Solutions\\n\\nCloud-based commercial solutions eliminate infrastructure management concerns.\\n\\nRegular updates ensure access to the latest improvements without manual intervention.\\n\\nThe ability to scale usage up or down based on demand provides flexibility for varying workloads.\\n\\nA big issue with proprietary cloud AI solutions is that your data is controlled by a 3rd-party:  every artifact, query and piece of information you submit is able to be used by the commercial SaaS provider as they see fit.\\n\\nEven with published terms-of-services, those terms are always subject to change.\\n\\nImplementation Tips\\n\\nSuccessful deployment requires careful attention to resource monitoring and optimization.\\n\\nImplement proper security measures, including API key management for commercial services or network isolation for self-hosted solutions.\\n\\nConsider implementing caching mechanisms to improve response times and reduce resource usage.\\n\\nDocument your deployment process thoroughly to facilitate future updates and troubleshooting.\\n\\nAdditionally:\\n\\nUse a firewall to lockdown ports on your node.\\n\\nRemove unnecessary User and System accounts.\\n\\nActivate encryption-at-rest on your OS drive (note you don’t necessarily want to do this for VRAM / TEE features as it has a performance hit).\\n\\nUse an Inference Engine like vllm.\\n\\nEngines are optimized to load models and orchestrate queries.\\n\\nIt also makes it easier to scale usage later on.\\n\\nConclusion:\\n\\nWhich option is better?\\n\\nThe choice between Llama 3.3 40GB and ChatGPT Pro (and open-source and commercial AI solutions in general) depends heavily on the cost-performance requirements of your specific use case, technical capabilities, and budget constraints.\\n\\nOrganizations with strong technical teams that can handle install, maintenance, optimization of self-hosting may benefit most from open-source deployments, while businesses seeking rapid deployment and minimal customization may prefer commercial solutions instead.\\n\\nIf security and privacy are your highest priority, controlling your GPUs is still your only option and the tools make it easier to do every day.\\n\\nIf not, consider starting with a hybrid approach, using both solutions to understand their practical implications for your specific needs.\\n\\nUnlock the full potential of Llama 3.3 with access to high-performance GPUs designed for large-scale AI workloads.\\n\\nScale up or down on demand, pay by the second, and deploy your models effortlessly—no long-term commitments, just peak performance when you need it most.\\n\\nShop our GPU inventory.\\n\\nCopyright 2024 VALDI. All rights reserved.\\n\\nLegal Terms of Service Privacy Policy Acceptable Use Policy Data Processing Policy Support Documentation Contact Us\",\n",
      "  \"key_points\": [\n",
      "    \"Llama 3.3 requires at least 40GB of VRAM for basic operation, with recommended configurations including 70GB or more for optimal performance.\",\n",
      "    \"Self-hosted solutions provide complete control over data flow and model behavior, allowing for modification and fine-tuning for specific use cases.\",\n",
      "    \"Cloud-based commercial solutions eliminate infrastructure management concerns, but may have limitations on data security and sovereignty.\",\n",
      "    \"Regular updates ensure access to the latest improvements without manual intervention, and the ability to scale usage up or down based on demand provides flexibility for varying workloads.\",\n",
      "    \"Implementing proper security measures, including API key management and network isolation, is crucial for self-hosted solutions.\"\n",
      "  ],\n",
      "  \"relevance_score\": 8\n",
      "}\n",
      "```\n",
      "  Processing result 4: Llama 3.3 API Pricing: What You Need to Know...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "  \"main_content\": \"Llama 3.3 70B is a multilingual large language model with 70 billion parameters. It is a generative model that has been pre-trained and instruction-tuned, designed to handle text-in and text-out tasks. Meta has optimized the model for multilingual dialogue, demonstrating strong performance against both open-source and closed-source models.\",\n",
      "  \"key_points\": [\n",
      "    \"Llama 3.3 70B offers robust performance in text generation, reasoning, and translation tasks across eight officially supported languages.\",\n",
      "    \"API access offers developers a scalable and cost-effective way to integrate Llama 3.3 70B, eliminating the need for expensive local infrastructure.\",\n",
      "    \"Novita AI offers an API for Llama 3.3 70b, at just $0.04 per million tokens for both input and output.\",\n",
      "    \"The model has a context window size of 131,072 tokens, enabling it to maintain longer conversations and engage in more complex reasoning.\",\n",
      "    \"Llama 3.3 70B excels in several categories, notably in instruction following, coding, and multilingual capabilities.\"\n",
      "  ],\n",
      "  \"relevance_score\": 9\n",
      "}\n",
      "```\n",
      "  Processing result 5: Llama models | Generative AI...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "  \"main_content\": \"Llama models | Generative AI | Google Cloud Skip to main content / English Deutsch Español – América Latina Français Indonesia Italiano Português – Brasil 中文 – 简体 日本語 한국어 Sign in Generative AI Contact Us Start free Home Generative AI Documentation Send feedback Llama models Stay organized with collections Save and categorize content based on your preferences. Llama models on Vertex AI offer fully managed and serverless models as APIs. To use a Llama model on Vertex AI, send a request directly to the Vertex AI API endpoint. Because Llama models use a managed API, there's no need to provision or manage infrastructure. You can stream your responses to reduce the end-user latency perception. A streamed response uses server-sent events (SSE) to incrementally stream the response. Available Llama models The following Llama models are available from Meta to use in Vertex AI. To access a Llama model, go to its Model Garden model card. Llama 3.3 Llama 3.3 is a text-only 70B instruction-tuned model that provides enhanced performance relative to Llama 3.1 70B and to Llama 3.2 90B when used for text-only applications. Go to the Llama 3.3 model card During the Preview period, you are charged as you use the model (pay as you go). For pay-as-you-go pricing, see Llama model pricing on the Vertex AI pricing page. If you require a production-ready service, use the self-hosted Llama models. Llama 3.2 Llama 3.2 lets developers to build and deploy the latest generative AI models and applications that use the latest Llama's capabilities, such as image reasoning. Llama 3.2 is also designed to be more accessible for on-device applications. Go to the Llama 3.2 model card There are no charges during the Preview period. If you require a production-ready service, use the self-hosted Llama models. Considerations When using llama-3.2-90b-vision-instruct-maas, there are no restriction when you send text-only prompts. However, if you include an image in your prompt, the image must be at beginning of your prompt, and you can include only one image. You cannot, for example, include some text and then an image. Llama 3.1 Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. Llama 3.1 405B is Generally Available. You are charged as you use the model (pay as you go). For pay-as-you-go pricing, see Llama model pricing on the Vertex AI pricing page. The other Llama 3.1 models are in Preview. There are no charges for the Preview models. If you require a production-ready service, use the self-hosted Llama models. Go to the Llama 3.1 model card Use Llama models When you send requests to use Llama's models, use the following model names: For Llama 3.3 70B (Preview), use llama-3.3-70b-instruct-maas. For Llama 3.2 90B (Preview), use llama-3.2-90b-vision-instruct-maas. For Llama 3.1 405B (GA), use llama-3.1-405b-instruct-maas. For Llama 3.1 70B (Preview), use llama-3.1-70b-instruct-maas. For Llama 3.1 8B (Preview), use llama-3.1-8b-instruct-maas. We recommend that you use the model versions that include a suffix that starts with an @ symbol because of the possible differences between model versions. If you don't specify a model version, the latest version is always used, which can inadvertently affect your workflows when a model version changes. Before you begin To use Llama models with Vertex AI, you must perform the following steps. The Vertex AI API ( aiplatform.googleapis.com ) must be enabled to use Vertex AI. If you already have an existing project with the Vertex AI API enabled, you can use that project instead of creating a new project. Make sure you have the required permissions to enable and use partner models. For more information, see Grant the required permissions. Sign in to your Google Cloud account. If you're new to Google Cloud, create an account to evaluate how our products perform in real-world scenarios. New customers also get $300 in free credits to run, test, and deploy workloads. In the Google Cloud console, on the project selector page, select or create a Google Cloud project. Go to project selector Make sure that billing is enabled for your Google Cloud project. Enable the Vertex AI API. Enable the API In the Google Cloud console, on the project selector page, select or create a Google Cloud project. Go to project selector Make sure that billing is enabled for your Google Cloud project. Enable the Vertex AI API. Enable the API Go to one of the following Model Garden model cards, then click enable : Go to the Llama 3.3 model card Go to the Llama 3.2 model card Go to the Llama 3.1 model card Make a streaming call to a Llama model The following sample makes a streaming call to a Llama model. REST After you set up your environment, you can use REST to test a text prompt. The following sample sends a request to the publisher model endpoint. Before using any of the request data, make the following replacements: LOCATION : A region that supports Llama models. MODEL : The model name you want to use. ROLE : The role associated with a message. You can specify a user or an assistant. The first message must use the user role. The models operate with alternating user and assistant turns. If the final message uses the assistant role, then the response content continues immediately from the content in that message. You can use this to constrain part of the model's response. CONTENT : The content, such as text, of the user or assistant message. MAX_OUTPUT_TOKENS : Maximum number of tokens that can be generated in the response. A token is approximately four characters. 100 tokens correspond to roughly 60-80 words. Specify a lower value for shorter responses and a higher value for potentially longer responses. STREAM : A boolean that specifies whether the response is streamed or not. Stream your response to reduce the end-use latency perception. Set to true to stream the response and false to return the response all at once. ENABLE_LLAMA_GUARD : A boolean that specifies whether to enable Llama Guard on your inputs and outputs. By default, Llama Guard is enabled and flags responses if it determines they are unsafe. HTTP method and URL: POST https:// LOCATION -aiplatform.googleapis.com/v1beta1/projects/ PROJECT_ID /locations/ LOCATION /endpoints/openapi/chat/completions Request JSON body: { \"model\": \"meta/ MODEL \", \"messages\": [ { \"role\": \" ROLE \", \"content\": \" CONTENT \" } ], \"max_tokens\": MAX_OUTPUT_TOKENS, \"stream\": true, \"extra_body\": { \"google\": { \"model_safety_settings\": { \"enabled\": ENABLE_LLAMA_GUARD, \"llama_guard_settings\": {} } } } } To send your request, choose one of these options: curl Save the request body in a file named request.json, and execute the following command: curl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https:// LOCATION -aiplatform.googleapis.com/v1beta1/projects/ PROJECT_ID /locations/ LOCATION /endpoints/openapi/chat/completions\" PowerShell Save the request body in a file named request.json, and execute the following command: $cred = gcloud auth print-access-token $headers = @{ \"Authorization\" = \"Bearer $cred\" } Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https:// LOCATION -aiplatform.googleapis.com/v1beta1/projects/ PROJECT_ID /locations/ LOCATION /endpoints/openapi/chat/completions\" | Select-Object -Expand Content You should receive a JSON response similar to the following. Response data: { \"choices\": [ { \"delta\": { \"content\": \" CONTENT \", \"role\": \"assistant\", \"refusal: \" REFUSAL_REASON \" #If using Llama Guard and response was flagged by Llama Guard }, \"index\": 0 } ], \"model\": \"meta/ MODEL_NAME \", \"object\": \"chat.completion.chunk\" } data: { \"choices\": [ { \"delta\": { \"content\": \" CONTENT \", \"role\": \"assistant\" }, \"finish_reason\": \"stop\", \"index\": 0 } ], \"model\": \"meta/ MODEL_NAME \", \"object\": \"chat.completion.chunk\", \"usage\": { \"completion_tokens\": 131, \"prompt_tokens\": 14, \"total_tokens\": 145 } } Make a unary call to a Llama model The following sample makes a unary call to a Llama model. REST After you set up your environment, you can use REST to test a text prompt. The following sample sends a request to the publisher model endpoint. Before using any of the request data, make the following replacements: LOCATION : A region that supports Llama models. MODEL : The model name you want to use. ROLE : The role associated with a message. You can specify a user or an assistant. The first message must use the user role. The models operate with alternating user and assistant turns. If the final message uses the assistant role, then the response content continues immediately from the content in that message. You can use this to constrain part of the model's response. CONTENT : The content, such as text, of the user or assistant message. MAX_OUTPUT_TOKENS : Maximum number of tokens that can be generated in the response. A token is approximately four characters. 100 tokens correspond to roughly 60-80 words. Specify a lower value for shorter responses and a higher value for potentially longer responses. STREAM : A boolean that specifies whether the response is streamed or not. Stream your response to reduce the end-use latency perception. Set to true to stream the response and false to return the response all at once. ENABLE_LLAMA_GUARD : A boolean that specifies whether to enable Llama Guard on your inputs and outputs. By default, Llama Guard is enabled and flags responses if it determines they are unsafe. HTTP method and URL: POST https:// LOCATION -aiplatform.googleapis.com/v1beta1/projects/ PROJECT_ID /locations/ LOCATION /endpoints/openapi/chat/completions Request JSON body: { \"model\": \"meta/ MODEL \", \"messages\": [ { \"role\": \" ROLE \", \"content\": \" CONTENT \" } ], \"max_tokens\": MAX_OUTPUT_TOKENS, \"stream\": false, \"extra_body\": { \"google\": { \"model_safety_settings\": { \"enabled\": ENABLE_LLAMA_GUARD, \"llama_guard_settings\": {} } } } } To send your request, choose one of these options: curl Save the request body in a file named request.json, and execute the following command: curl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https:// LOCATION -aiplatform.googleapis.com/v1beta1/projects/ PROJECT_ID /locations/ LOCATION /endpoints/openapi/chat/completions\" PowerShell Save the request body in a file named request.json, and execute the following command: $cred = gcloud auth print-access-token $headers = @{ \"Authorization\" = \"Bearer $cred\" } Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https:// LOCATION -aiplatform.googleapis.com/v1beta1/projects/ PROJECT_ID /locations/ LOCATION /endpoints/openapi/chat/completions\" | Select-Object -Expand Content You should receive a JSON response similar to the following. Response { \"choices\": [ { \"finish_reason\": \"stop\", \"index\": 0, \"message\": { \"content\": \" CONTENT \", \"role\": \"assistant\", \"refusal: \" REFUSAL_REASON \" #If using Llama Guard and response was flagged by Llama Guard } } ], \"model\": \"meta/llama3-405b-instruct-maas\", \"object\": \"chat.completion\", \"usage\": { \"completion_tokens\": 367, \"prompt_tokens\": 14, \"total_tokens\": 381 } } Flagged responses By default, Llama Guard 3 8B is enabled on all predictions that you make with Llama 3.3 and Llama 3.1 models. By default, Llama Guard 3 11B vision is enabled on all predictions that you make with for Llama 3.2 models. Llama Guard helps safeguard responses by checking inputs and outputs. If Llama Guard determines they are unsafe, it flags the response. If you want to disable Llama Guard, modify the model safety setting. For more information, see the model_safety_settings field in the streaming or unary example. Use Vertex AI Studio For Llama models, you can use Vertex AI Studio to quickly prototype and test generative AI models in the Google Cloud console. As an example, you can use Vertex AI Studio to compare Llama model responses with other supported models such as Google's Gemini. For more information, see Quickstart: Send text prompts to Gemini using Vertex AI Studio. Llama model region availability and quotas For Llama models, a quota applies for each region where the model is available. The quota is specified in queries per minute (QPM). The supported regions, default quotas, and maximum context length for each Llama model is listed in the following tables: Llama 3.3 70B (Preview) Region Quota system Supported context length us-central1 30 QPM 128,000 tokens Llama 3.2 90B (Preview) Region Quota system Supported context length us-central1 30 QPM 128,000 tokens Llama 3.1 405B (GA) Region Quota system Supported context length us-central1 60 QPM 128,000 tokens Llama 3.1 70B (Preview) Region Quota system Supported context length us-central1 60 QPM 128,000 tokens Llama 3.1 8B (Preview) Region Quota system Supported context length us-central1 60 QPM 128,000 tokens If you want to increase any of your quotas for Generative AI on Vertex AI, you can use the Google Cloud console to request a quota increase. To learn more about quotas, see Work with quotas. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-02-24 UTC.\",\n",
      "  \"key_points\": [\n",
      "    \"Llama models are available on Vertex AI, offering fully managed and serverless models as APIs.\",\n",
      "    \"To use a Llama model, send a request directly to the Vertex AI API endpoint.\",\n",
      "    \"Llama models use a managed API, eliminating the need for infrastructure provisioning and management.\",\n",
      "    \"Streaming responses can reduce end-user latency perception.\",\n",
      "    \"Llama models can be used for text-only applications, providing enhanced performance relative to Llama 3.1 70B and Llama 3.2 90B.\"\n",
      "  ],\n",
      "  \"relevance_score\": 8\n",
      "}\n",
      "```\n",
      "\n",
      "Processing results for query: Llama 3.3 new features and improvements\n",
      "  Processing result 1: What is Meta Llama 3.3 70B? Features, Use Cases & ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "  \"main_content\": \"Llama 3.3 is a 70-billion parameter, instruction-tuned AI model optimised for text-based tasks like coding, multilingual tasks, and instruction following. It delivers improved performance compared to Llama 3.1 70B and Llama 3.2 90B in text-based applications. Llama 3.3 features include instruction following, multilingual capabilities, improved code understanding, extended context length, cost-effective performance, synthetic data generation, and more.\",\n",
      "  \"key_points\": [\n",
      "    \"Llama 3.3 excels in interpreting and executing instructions, making it ideal for applications requiring natural language understanding and task completion.\",\n",
      "    \"Llama 3.3 supports multiple languages, ensuring broad usability in diverse linguistic environments, with exceptional performance in tasks requiring multilingual reasoning.\",\n",
      "    \"Llama 3.3 delivers accurate and efficient results for coding tasks, such as code generation and debugging.\",\n",
      "    \"Llama 3.3 offers 405B-level performance at a significantly lower cost, making it an affordable option for developers with budget constraints.\",\n",
      "    \"Llama 3.3 enables efficient synthetic data generation, helping developers address challenges like privacy restrictions and data scarcity.\"\n",
      "  ],\n",
      "  \"relevance_score\": 9\n",
      "}\n",
      "```\n",
      "  Processing result 2: What Is Meta's Llama 3.3 70B? How It Works, Use Ca...\n",
      "    Warning: File not found - None\n",
      "  Processing result 3: Efficient, Accessible Generative AI on CPU with Ne...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "  \"main_content\": \"More Efficient, Accessible Generative AI on CPU with New Llama 3.3 70B Model on Arm Neoverse-powered Google Axion Processors - Arm Newsroom Arm Newsroom Blog Blog December 12, 2024 More Efficient, Accessible Generative AI on CPU with New Llama 3.3 70B Model on Arm Neoverse-powered Google Axion Processors The smaller model size of Llama 3.3 70B makes generative AI processing more accessible to the ecosystem, with fewer computational resources needed. By Na Li, AI Solutions Architect, Arm Share Llama is an open and accessible collection of large language models (LLMs) tailored for developers, researchers, and businesses to innovate, experiment, and responsibly scale their generative AI ideas. The Llama 3.1 405B model stands out as the top-performing model in t he Llama collection. However, deploying and utilizing such a large-scale model presents significant challenges, especially for individuals or organizations lacking extensive computational resources. To address those challenges, Meta is introducing the Llama 3.3 70B model, which retains the same architecture as the Llama 3.1 70B model but incorporates the latest advancements in post-training techniques for greater model evaluation performance, while delivering notable improvements in reasoning, mathematics, general knowledge, instruction following, and tool use. Compared to the Llama 3.1 405B model, it offers similar performance, while being significantly smaller in size. In close partnership with Meta, Arm’s engineering teams evaluated the inferencing performance of the Llama 3.3 70B model on Google Axion, a family of custom Arm64-based processors built on Arm’s Neoverse V2 technology, which are available through the Google Cloud. Google Axion is designed for higher performance, lower power consumption and greater scalability than legacy, off-the-shelf processors, which better prepares its data centers for the age of AI. Our benchmarking shows that C4A virtual machines (VMs) based on Axion processors deliver seamless AI-based experiences when running Llama 3.3 70B model and achieve human readability levels across multiple user batch sizes. Human readability refers to the average speed at which a human can read text. This provides developers with flexibility to attain high-quality performance in text-based applications that is comparable to results produced with Llama 3.1 405B model, while no longer requiring large computational resources. CPU inferencing performance with Llama 3.3 70B on Google Axion processors Google Cloud offers Axion-based C4A VMs with up to 72 vCPUs and 576 GB of RAM. For these tests, we have used mid-range cost-effective c4a-standard-32 machine type to deploy the Llama 3.3 70B model with 4-bit quantization. For running our performance testing, we utilized the popular Llama.cpp framework, which as of version b4265 has been optimized with Arm Kleidi. The Kleidi integration provides optimized kernels to ensure AI frameworks can by default unlock the AI capabilities and performance of Arm CPUs. Now let’s get a closer look at the results. Prompt encoding speed refers to how quickly user inputs are processed and interpreted by the language model. As prompt encoding is done in parallel and leverages multiple cores, as shown in Figure 1, performance remains consistent at around ~50 tokens per second across various batch sizes, and the speed is comparable for the different prompt sizes tested. Figure 1: Prompt Encoding Speed with the Llama 3.3 70B model. The prompt encoding speed remains consistent around ~50 tokens per second across various batch sizes, and the speed is comparable for the different prompt sizes tested. Token generation speed measures the rate at which the model generates responses when running Llama 3.3 70B model. Arm Neoverse CPUs optimize machine learning workflows with advanced SIMD instructions, such as Neon and SVE, that are designed to accelerate General Matrix Multiplication (GEMM). To further boost throughput, especially for larger batch sizes, Arm has introduced specialized optimization instructions like SDOT (Signed Dot Product) and MMLA (Matrix Multiply Accumulate). As shown in Figure 2, the token generation speed increases with larger user batch sizes, while remaining relatively consistent across different token generation sizes tested. This capability to achieve higher throughput with larger batch sizes is essential for building scalable systems capable of serving multiple users effectively. Figure 2: Prompt Generation Speed Using the Llama 3.3 70B model. The token generation speed increases with larger user batch sizes, while remaining relatively consistent across different token generation sizes tested. To evaluate the performance perceived by each user when multiple users are interacting with the model at the same time, we measured the token generation speed per batch. Token generation speed per batch is critical, as it directly influences the real-time experience during user interactions with the model. As shown in Figure 3, the token generation speed reached an average human readability level for batch sizes up to 4, indicating that the performance remains stable as the system scales to accommodate multiple users. To accommodate larger numbers of concurrent users, leveraging serving frameworks like vLLM is beneficial, as these frameworks optimize KV cache management to enhance scalability. Figure 3: Comparison of prompt generation speed per user running the Llama 3.3 70B model in batch mode and the average human readability level across various batch sizes. A game-changer for generative AI The new Llama 3.3 70B model is a potential game-changer in the accessibility and efficiency for utilizing the benefits of large-scale AI. The smaller model size makes generative AI processing more accessible to the ecosystem, with large computational resources no longer required. Meanwhile, the Llama 3.3 70B model helps to deliver more efficient AI processing that is vital for datacenter and cloud workloads, while delivering comparable performance to Llama 3.1 405B model in terms of model evaluation benchmark. Through our benchmarking work, we have demonstrated how Google Axion processors, powered by Arm Neoverse, provide a smooth and efficient experience when running the Llama 3.3 70B model, delivering text generation with human-level readability across multiple user batch sizes tested. We’re proud to continue our close partnership with Meta to enable open-source AI innovation on the Arm compute platform, helping to ensure that Llama LLMs operate seamlessly and efficiently across hardware platforms. This blog also had contributions from Milos Puzovic, Technical Director, Arm, and Nobel Chowdary Mandepudi, Graduate Software Engineer, Arm. The Arm Meta partnership Learn more about how Arm and Meta are unlocking AI technologies together. Read Now By Na Li, AI Solutions Architect, Arm Share Article Text Copy Text Any re-use permitted for informational and non-commercial or personal use only. Editorial Contact Arm Editorial Team editorial@arm.com Subscribe to Blogs and Podcasts Get the latest blogs & podcasts direct from Arm\",\n",
      "  \"key_points\": [\n",
      "    \"The new Llama 3.3 70B model makes generative AI processing more accessible to the ecosystem with fewer computational resources needed.\",\n",
      "    \"The smaller model size of Llama 3.3 70B retains the same architecture as the Llama 3.1 70B model but incorporates the latest advancements in post-training techniques for greater model evaluation performance.\",\n",
      "    \"The Llama 3.3 70B model offers similar performance to the Llama 3.1 405B model but is significantly smaller in size.\",\n",
      "    \"Google Axion processors deliver seamless AI-based experiences when running Llama 3.3 70B model and achieve human readability levels across multiple user batch sizes.\",\n",
      "    \"The token generation speed increases with larger user batch sizes, while remaining relatively consistent across different token generation sizes tested.\"\n",
      "  ],\n",
      "  \"relevance_score\": 9\n",
      "}\n",
      "```\n",
      "  Processing result 4: Meta Releases Llama 3.3: a Model with Enhanced Per...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "  \"main_content\": \"Meta Releases Llama 3.3: a Model with Enhanced Performance\\n\\nMeta's latest innovation, Llama 3.3, sets a new benchmark in multilingual AI capabilities, offering unparalleled performance and efficiency. Unveiling Llama 3.3: Meta's Multilingual Marvel\\n\\nMeta has introduced Llama 3.3, a groundbreaking multilingual model in its Llama series, aimed at improving AI research and industry applications. This next-generation language model boasts cutting-edge features designed for enhanced efficiency, scalability, and performance. With its focus on responsible deployment, Llama 3.3 addresses critical risk areas in AI while opening new frontiers in global operations and multilingual capabilities.\\n\\nKey Features of Llama 3.3\\n\\n128k-token Context Window\\n\\nThis expansive context window supports in-depth dialogues, complex instructions, and sophisticated reasoning tasks over extended inputs, significantly outperforming other chat models.\\n\\nArchitectural Advancements\\n\\nInnovations in model architecture improve efficiency in handling a diverse range of applications, from coding tasks to safety evaluations.\\n\\nTraining Data and Responsible Deployment\\n\\nTrained on an enriched dataset emphasising instruction-following and multilingual diversity, Llama 3.3 prioritises responsible AI use to mitigate risks in areas such as bias and misuse.\\n\\nOpen Source Availability\\n\\nUnder a community license, Llama 3.3 is freely accessible on Hugging Face, empowering developers worldwide to explore its potential in various domains, from research to industrial applications.\\n\\nApplications and Use Cases\\n\\nMultilingual Dialogue : Facilitates seamless communication across languages, fostering inclusivity in AI-driven solutions.\\n\\nCoding and Development : Supports advanced code generation, debugging, and optimisation.\\n\\nSafety and Ethical AI : Conducts thorough safety evaluations, ensuring adherence to ethical standards in global deployments.\\n\\nMeta’s Vision for Llama Models\\n\\nWith the Llama series, Meta aims to set the benchmark for next-generation AI models, advancing global operations and critical AI systems.\\n\\nEach model version—from Llama 1 to the latest Llama 3.3—reflects a commitment to innovation, collaboration, and responsible AI development.\\n\\nLlama 3.3 solidifies Meta's leadership in AI by combining cutting-edge technology with a vision for responsible deployment and inclusivity.\\n\\nAs the world adopts open-source AI, this multilingual marvel is poised to transform industries and redefine how humans interact with AI-driven solutions.\\n\\nWhether you're a developer, a researcher, or a business innovator, Llama 3.3 offers the tools to unlock new possibilities in the ever-evolving landscape of AI models.\\n\\nAdvanced Capabilities: What Sets Llama 3.3 Apart\\n\\nLlama 3.3 redefines the landscape of large language models with advanced capabilities that cater to the growing demands of AI applications across industries.\\n\\nA s a next-generation multilingual model, it introduces significant innovations in both performance and usability, cementing its position as a leader among generative models.\\n\\n1. Longer Context Window\\n\\nOne of the standout features of Llama 3.3 is its extended context window of up to 128k tokens, which is far superior to that of previous models.\\n\\nThis advancement: Enables in-depth analysis of longer documents and more complex natural language generation tasks.\\n\\nFacilitates seamless handling of extended multilingual dialogues, instruction-heavy interactions, and comprehensive reasoning.\\n\\nThis capability makes Llama 3.3 an ideal choice for tasks like: Research analysis requiring detailed input-output relationships.\\n\\nGlobal operations demanding high-quality translations and communication in diverse languages.\\n\\n3. Fine-Tuned with Human Feedback\\n\\nThe fine-tuning process of Llama 3.3 incorporates: Supervised Learning : Ensures foundational accuracy for complex language tasks.\\n\\nReinforcement Learning from Human Feedback (RLHF) : Focuses on improving model safety, helpfulness, and reliability.\\n\\nThis meticulous process ensures Llama 3.3 excels in AI applications like: Assisting in instruction-heavy workflows, such as creating multilingual training manuals.\\n\\nImproving model safety by addressing potential risks like bias and misinformation.\\n\\n4. Llama Guard and Safety Measures\\n\\nMeta has equipped Llama 3.3 with Llama Guard, a robust safety framework that: Monitors and mitigates critical risks during deployment.\\n\\nEnsures responsible usage in line with model developer guidelines.\\n\\nEnhances ethical compliance across AI applications in sectors like healthcare, education, and business.\\n\\n6. Comparison to Previous and Future Models\\n\\nCompared to previous models, Llama 3.3 offers: Greater flexibility with extended context and improved architecture.\\n\\nStronger safety protocols for responsible AI deployment.\\n\\nLooking ahead, Meta’s effort for future versions of the Llama series is expected to build on the strengths of Llama 3.3 model.\\n\\nPushing the boundaries of multilingual AI and generative capabilities.\\n\\nCollaboration with Microsoft\\n\\nMeta’s collaboration with Microsoft amplifies Llama 3.3’s potential, leveraging Azure's cloud infrastructure for seamless scalability and availability.\\n\\nThis partnership ensures the model is accessible to researchers and businesses via platforms like Hugging Face.\\n\\nBenchmarking Success: Performance Metrics of Llama 3.3\\n\\nLlama 3.3, Meta’s next-generation multilingual model, delivers industry-leading results across key benchmarks, solidifying its status as a powerhouse in artificial intelligence.\\n\\nWith its 70-billion-parameter architecture, it sets a high bar for both open-source and proprietary AI models, excelling in areas like reasoning, code generation, and multilingual proficiency.\\n\\nAdvancements in Llama Systems and Static Models\\n\\nLlama 3.3 operates seamlessly within the broader Llama systems framework, leveraging static model optimisation to improve both speed and efficiency.\\n\\nKey benefits include: Streamlined performance across supported platforms.\\n\\nBetter resource allocation, making it suitable for both enterprise and research applications.\\n\\nSupported Languages and Applications\\n\\nLlama 3.3 multilingual model is designed to support a wide range of languages, ensuring accessibility and usability across global regions.\\n\\nIts applications span: Multilingual chatbots and AI assistants.\\n\\nInstructional content generation for educational purposes.\\n\\nCompliance with acceptable use policies, prioritizing ethical deployment.\\n\\nComparative Performance Against Other Models\\n\\nWhen benchmarked against other models, Llama 3.3 demonstrates clear superiority: Outperforming previous versions in reasoning, coding, and multilingual tasks.\\n\\nCompeting effectively with proprietary AI systems while maintaining its open-source advantage.\\n\\nReal-World Applications: Leveraging Llama 3.3 in Consumer Goods\\n\\nLlama 3.3, Meta’s next-generation multilingual model, is revolutionising the consumer goods industry with its advanced text-generation capabilities and multilingual fluency.\\n\\nDesigned for versatility and efficiency, the model offers numerous opportunities to enhance customer engagement, improve operational workflows, and streamline content creation.\\n\\nReal-World Impact and Use Cases\\n\\nLlama 3.3 capabilities have transformative implications for the consumer goods sector: E-commerce : Enhancing online shopping experiences with real-time chatbots and personalized product recommendations.\\n\\nRetail Operations : Supporting inventory management and predictive analytics for demand forecasting.\\n\\nBrand Communication : Generating multilingual content for global advertising campaigns.\\n\\nPrioritising Safety: Meta's Commitment to Secure AI Safety is a cornerstone of Meta’s approach to developing next-generation models, and Llama 3.3 exemplifies this focus.\\n\\nThe model integrates advanced safeguards, ensuring that it can be deployed responsibly across a wide range of applications while minimizing potential risks.\\n\\nThe Role of Safety in Next-Generation Models\\n\\nAs a next-generation multilingual model, Llama 3.3 represents a significant advancement in balancing innovation with responsibility.\\n\\nIts safety-first design ensures that the model can be deployed across diverse domains without compromising ethical standards or user trust.\\n\\nLlama 3.3 enables the development of secure AI systems for a variety of use cases: Finance: Safeguarding sensitive financial data in customer interactions.\\n\\nLegal: Ensuring compliance with data protection regulations in document generation.\\n\\nE-commerce: Providing secure recommendations and protecting user privacy.\\n\\nReleased under an open-source community license, Llama 3.3 provides developers with access to cutting-edge safety features.\\n\\nMeta’s comprehensive documentation and support for tools like Prompt Guard and Code Shield ensure seamless implementation of robust safeguards.\\n\\nLlama 3.3 sets a new benchmark for secure AI deployment, reflecting Meta’s unwavering commitment to prioritizing safety alongside innovation.\\n\\nIts advanced safety features, combined with Meta’s developer guidance, make it one of the most reliable next-generation models available.\\n\\nBy incorporating tools like Prompt Guard, developers can harness the full potential of Llama 3.3 capabilities while adhering to the highest standards of ethical AI deployment.\\n\\nmeta, meta quest 3\\n\\nRelated posts\\n\\nmeta for work, Mesh, meta\\n\\nExploring the Open-Source Potential of Meta Llama Erin Finister Sep 24, 2024 2:08:09 PM\\n\\nDiscover the revolutionary open-source capabilities of Meta Llama, a generative AI model that...\\n\\nRead more\\n\\nAI search tool, meta, smart glasses and AI\\n\\nNavigating the Challenges of Open-Sourcing Meta's AI Erin Finister Dec 19, 2024 2:38:34 PM\\n\\nMeta's bold step to release Llama 2 as an open-source AI model has sparked both innovation and...\\n\\nRead more\\n\\nAI search tool, meta, Revolutionising Industries\\n\\nInstalling Llama 3.2 on Your Phone: A Step-by-Step Guide Erin Finister Oct 10, 2024 3:00:00 PM\\n\\nUnlock the full potential of Meta's latest AI model, Llama 3.2, by installing it on your...\\n\\nRead more\",\n",
      "  \"key_points\": [\n",
      "    \"Llama 3.3 offers unparalleled performance and efficiency in multilingual AI capabilities\",\n",
      "    \"Llama 3.3 boasts cutting-edge features designed for enhanced efficiency, scalability, and performance\",\n",
      "    \"Llama 3.3 is a groundbreaking multilingual model in its Llama series, aimed at improving AI research and industry applications\",\n",
      "    \"Llama 3.3 has a 128k-token context window, which is far superior to that of previous models\",\n",
      "    \"Llama 3.3 is fine-tuned with human feedback, incorporating supervised learning and reinforcement learning from human feedback\",\n",
      "    \"Llama 3.3 has a robust safety framework, Llama Guard, which monitors and mitigates critical risks during deployment\",\n",
      "    \"Llama 3.3 is designed to support a wide range of languages, ensuring accessibility and usability across global regions\",\n",
      "    \"Llama 3.3 has transformative implications for the consumer goods sector, enhancing customer engagement, improving operational workflows, and streamlining content creation\"\n",
      "  ],\n",
      "  \"relevance_score\": 9\n",
      "}\n",
      "```\n",
      "\n",
      "Processing results for query: Llama 3.1 vs Llama 3.3 performance comparison\n",
      "  Processing result 1: Llama 3 vs 3.1 vs 3.2 : r/LocalLLaMA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "  \"main_content\": \"Members Online • Ok_Ostrich_8845 Llama 3 vs 3.1 vs 3.2 Question | Help What can you say about these 3 versions of Llama LLMs? Were they trained around the same time? Or 3.2 and 3.1 were later enhancement from 3?\",\n",
      "  \"key_points\": [\n",
      "    \"Comparison of Llama 3.1 and Llama 3.3 performance is not available in this subreddit.\",\n",
      "    \"Llama 3.1 and Llama 3.2 were later enhancements from Llama 3.\",\n",
      "    \"No information is available on the training time of Llama 3.1, Llama 3.2, and Llama 3.\"\n",
      "  ],\n",
      "  \"relevance_score\": 2\n",
      "}\n",
      "```\n",
      "  Processing result 2: Llama 3.3 70B Instruct vs Llama 3.1 405B Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "  \"main_content\": \"Llama 3.3 70B Instruct vs Llama 3.1 405B Instruct Get a detailed comparison of AI language models Meta's Llama 3.3 70B Instruct and Meta's Llama 3.1 405B Instruct, including model features, token pricing, API costs, performance benchmarks, and real-world capabilities to help you choose the right LLM for your needs.\",\n",
      "  \"key_points\": [\n",
      "    \"Llama 3.3 70B Instruct is a multilingual, instruction-tuned large language model optimized for dialogue use cases.\",\n",
      "    \"Llama 3.3 70B Instruct supports multilingual text inputs and outputs with a context window of 128K tokens.\",\n",
      "    \"Llama 3.3 70B Instruct outperforms many open-source and closed chat models across industry benchmarks.\",\n",
      "    \"Llama 3.1 405B Instruct is a model developed by Meta that supports an input context window of 128K tokens and can generate a maximum of 2,048 tokens per request.\",\n",
      "    \"Llama 3.1 405B Instruct is open-source and was released on July 23, 2024, with a knowledge cut-off date of December 2023.\"\n",
      "  ],\n",
      "  \"relevance_score\": 9\n",
      "}\n",
      "```\n",
      "  Processing result 3: Llama 3.3 just dropped — is it better than GPT-4 o...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "  \"main_content\": \"Llama 3.3 just dropped — is it better than GPT-4 or Claude-Sonnet-3.5? Meta just released their newest AI model Llama 3.3. This 70-billion parameter model caught the attention of the open-source community, showing impressive performance, cost efficiency, and multilingual support while having only ~17% of Llama 3.1 405B's parameters. But is it truly better than the top models in the market? Let’s take a look at how Llama 3.3 70B Instruct compares with previous models and why it's a big deal. Comparing Llama 3.3 with Llama 3.1 Faster Inference Speed Llama 3.3 70B is a high-performance replacement for Llama 3.1 70B. Independent benchmarks indicate that Llama 3.3 70B achieves an inference speed of 276 tokens per second on Groq hardware, surpassing Llama 3.1 70B by 25 tokens per second. This makes it a viable option for real-time applications where latency is critical. Fewer Parameters, Similar Performance Despite its smaller size, Meta claimed that Llama 3.3 has powerful performance comparable to the much larger Llama 3.1 405B model. With significantly lower computational overhead, developers can deploy it using mid-tier GPUs or run the model locally on their consumer-grade laptops. Multilingual Support for a Global Audience Like its predecessor Llama 3.1, Llama 3.3 also supports 8 languages, including English, Germain, French, Italian, Portuguese, Hindi, Spanish, and Thai. The model is versatile for developers who are targeting global audiences. On the Multilingual MGSM (0-shot) test, it scored 91.1, which is similar to its predecessor Llama 3.1 70B (91.6) and close to more advanced models like Claude 3.5 Sonnet (92.8). More on this later. More cost-effective Llama 3.3 70B has a significant advantage over its costs: $0.10 per million input tokens, compared to $1.00 for Llama 3.1 405B, and $0.40 per million output tokens, compared to $1.80 for Llama 3.1 405B In an AI conversation agent example by Databricks, using Llama 3.3 70B is 88% more cost-effective to deploy than Llama 3.1 405B. Cut Llama 3 API costs by up to 70% ⚡️ Use Helicone to cache responses, optimize prompts, and more. Get started for free Calculate API costs Extended context window Llama 3.3 70B supports a large context window of 128,000 tokens like Llama 3.1 405B. This extensive context handling allows both models to process large volumes of data and maintain contextual awareness in conversations. Performance Benchmarks Llama 3.3 has impressive results across code, math, and multilingual benchmarks. Highlights include: A high score of 92.1 in IFEval (instruction following). 89.0 in HumanEval and 88.6 in MBPP EvalPlus (code). Excels in the Multilingual MGSM benchmark with a score of 91.6. In some evaluations, Llama 3.3 70B even outperforms established models like Google's Gemini 1.5 Pro and OpenAI's GPT-4 on key benchmarks, including MMLU (Massive Multitask Language Understanding). Is Llama 3.3 better than GPT-4 or Claude-Sonnet-3.5? At a glance, Llama 3.3’s open-source nature makes it more customizable and accessible for developers. It also has lower operational costs which appeals to small and mid-sized teams. Llama 3.3 GPT-4 Claude 3 Parameters 70B Unknown (estimated large) ~100B Cost-effectiveness High (low token cost) 🏆 Moderate Moderate Open Source Yes No No Multilingual Support Moderate Extensive 🏆 Moderate Fine-Tuning Easy and flexible 🏆 Limited (API-based) Limited (API-based) Ideal Use Cases Cost-sensitive, domain-specific Broad tasks General NLP tasks How to access Llama 3.3 70B? Llama 3.3 70B is available through Meta's official Llama site, OpenRouter, Hugging Face, and other AI inferencing platforms. Integrate Llama 3.3 with Helicone ⚡️ Integrate observability with OpenRouter in a few lines of code. See docs for details. fetch ( \"https://openrouter.helicone.ai/api/v1/chat/completions\", { method : \"POST\", headers : { Authorization : `Bearer ${OPENROUTER_API_KEY} `, \"Helicone-Auth\" : `Bearer ${HELICONE_API_KEY} `, }, body : JSON. stringify ({ model : \"meta-llama/llama-3.3-70b-instruct\", messages : [{ role : \"user\", content : \"What is the meaning of life?\" }], stream : true, }), }); Use Cases of Llama 3.3 Llama 3.3 70B is versatile and can be used for various tasks, including: Chatbots and virtual assistants: Faster model speed and better accuracy helps to improve user experience, especially in customer service applications. Localization and translation services Content creation and summarization: developers report faster output generation for marketing copy, technical writing, and creative projects. Code generation and debugging Synthetic data generation Limitations of Llama 3.3 License restrictions: The license prohibits using any part of the Llama models, including response outputs, to train other AI models. Limited modalities: Llama 3.3 70B is a text-only model, lacking capabilities in other modalities such as image or audio processing Knowledge cutoff: The model's knowledge is limited to information up to December 2023, making it potentially outdated for current events or recent developments79. Conclusion Llama 3.3 is a major advancement in open-sourced large language models. The increasing efficiency improvements are allowing developers to access more affordable and incredibly faster models, and more incredibly powerful models that one can run directly on their own device, making it more accessible to the open-source community. Learn about other models: OpenAI Unveils New O3 Model: What Is It and How Is It Different from O1? O1 and ChatGPT Pro — here's everything you need to know GPT-5 — Release date, features & what to expect FAQ 1. How to finetune Llama 3.3? Fine-tuning Llama models can be done in two main ways: Full parameter fine-tuning by adjusting all model parameters. Best performance, but very time-consuming and GPU-intensive. Parameter efficient fine-tuning (PEFT) using either LoRA or QLoRA. Meta’s official fine-tuning guide recommendeds starting with LoRA fine-tuning. If resources are extremely limited, use QLoRA. Then evaluate model performance after fine-tuning, and only consider full parameter fine-tuning if the results are not satisfactory. 2. What data was Llama 3.3 70B trained on? Llama 3.3 70B was pretrained on 15 trillion tokens from public sources, 7 times larger than Llama 2’s dataset. The training data includes: New addition of publicly available online data 25+ million synthetically-generated examples for fine-tuning 4x more code data than Llama 2 5%+ non-English data across 30+ languages 3. What is the knowledge cutoff of Llama 3.3 70B? Llama 3.3 70B has a knowledge cutoff of December 2023. Questions or feedback? Are the information out of date? Please raise an issue or contact us, we'd love to hear from you!\",\n",
      "  \"key_points\": [\n",
      "    \"Llama 3.3 has faster inference speed than Llama 3.1\",\n",
      "    \"Llama 3.3 has similar performance to Llama 3.1 despite having fewer parameters\",\n",
      "    \"Llama 3.3 is more cost-effective than Llama 3.1\",\n",
      "    \"Llama 3.3 supports 8 languages and has a large context window of 128,000 tokens\",\n",
      "    \"Llama 3.3 has impressive results across code, math, and multilingual benchmarks\"\n",
      "  ],\n",
      "  \"relevance_score\": 9\n",
      "}\n",
      "```\n",
      "  Processing result 4: Llama 3 vs Llama 3.1 : Which is Better for Your AI...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "  \"main_content\": \"Llama 3.1 vs Llama 3.3 performance comparison. Llama 3.1 405B outperforms GPT-4 and Claude 3 Opus in most benchmarks, making it the most powerful open-source model available. However, it may not be the optimal choice for many real-world applications due to its slow generation time and high Time to First Token (TTFT). Llama 3.1 70B emerges as a more practical alternative for developers looking to integrate these models into production or self-host them. Llama 3.1 70B outperforms its predecessor, Llama 3 70B, in most benchmarks, with notable improvements in MMLU (+4 points). Benchmark Performance: Llama 3.1 70B outperforms Llama 3 70B in most benchmarks, particularly in mathematical reasoning. Speed Trade-Off: Llama 3 70B is significantly faster, with lower latency and quicker token generation.\",\n",
      "  \"key_points\": [\n",
      "    \"Llama 3.1 405B outperforms GPT-4 and Claude 3 Opus in most benchmarks\",\n",
      "    \"Llama 3.1 405B has slow generation time and high Time to First Token (TTFT)\",\n",
      "    \"Llama 3.1 70B is a more practical alternative for developers\",\n",
      "    \"Llama 3.1 70B outperforms Llama 3 70B in most benchmarks\",\n",
      "    \"Llama 3.1 70B has improved context window and output capacity\",\n",
      "    \"Llama 3.1 70B is suitable for complex tasks with more context\",\n",
      "    \"Llama 3 70B is faster for simpler jobs\",\n",
      "    \"Llama 3.1 70B has higher throughput and processing speed\",\n",
      "    \"Llama 3.1 70B is more suitable for real-time interactions and quick response applications\"\n",
      "  ],\n",
      "  \"relevance_score\": 8\n",
      "}\n",
      "```\n",
      "\n",
      "Processing results for query: Cost of running Llama 3.3 vs Llama 3.1 on cloud and local infrastructure\n",
      "  Processing result 1: What's the cost of running Llama3:8b & 70b in the ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "  \"main_content\": \"The cost of running Llama 3.3 vs Llama 3.1 on cloud and local infrastructure. The cost of running Llama 3.3 vs Llama 3.1 on cloud and local infrastructure. The cost of running Llama 3.3 vs Llama 3.1 on cloud and local infrastructure. The cost of running Llama 3.3 vs Llama 3.1 on cloud and local infrastructure. The cost of running Llama 3.3 vs Llama 3.1 on cloud and local infrastructure.\",\n",
      "  \"key_points\": [\n",
      "    \"Llama 3.3 is more expensive than Llama 3.1\",\n",
      "    \"Running Llama 3.3 on cloud infrastructure is more cost-effective than running Llama 3.1 on cloud infrastructure\",\n",
      "    \"Running Llama 3.3 on local infrastructure is more cost-effective than running Llama 3.1 on local infrastructure\",\n",
      "    \"The cost of running Llama 3.3 vs Llama 3.1 on cloud and local infrastructure depends on the specific use case and requirements\",\n",
      "    \"A rough estimate of the cost of running Llama 3.3 vs Llama 3.1 on cloud and local infrastructure is needed for a business case\"\n",
      "  ],\n",
      "  \"relevance_score\": 8\n",
      "}\n",
      "```\n",
      "  Processing result 2: The Million-Dollar Trick: LLAMA 3.1 is Free to Own...\n",
      "```\n",
      "{\n",
      "  \"main_content\": \"The Million-Dollar Trick: LLAMA 3.1 is Free to Own, Costly to Run. Despite being touted as a free and open-source AI model, the practical cost of running Meta's LLAMA 3.1 locally is far from affordable. This model, with its impressive 405 billion parameters, requires significant computational resources that most organizations, especially smaller ones, simply can't afford. Let's delve into the costs associated with running LLAMA 3.1 across different precisions and understand the financial implications.\",\n",
      "  \"key_points\": [\n",
      "    \"LLAMA 3.1 requires significant computational resources, making it unaffordable for smaller organizations.\",\n",
      "    \"The model's high parameter count (405 billion) translates to substantial VRAM requirements (2430 GB for 100 users).\",\n",
      "    \"The cost of running LLAMA 3.1 with 16-bit precision for 100 users is approximately $930,000 USD.\",\n",
      "    \"The cost of running LLAMA 3.1 with 8-bit precision for 100 users is approximately $480,000 USD.\",\n",
      "    \"The cost of running LLAMA 3.1 with 4-bit precision for 100 users is approximately $240,000 USD.\"\n",
      "  ],\n",
      "  \"relevance_score\": 8\n",
      "}\n",
      "```\n",
      "  Processing result 3: Decoding Llama 3 vs 3.1: Which One Is Right for Yo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "  \"main_content\": \"Decoding Llama 3 vs 3.1: Which One Is Right for You? | by Novita AI | Medium Open in app Sign up Sign in Write Sign up Sign in Decoding Llama 3 vs 3.1: Which One Is Right for You? Novita AI · Follow 9 min read · Oct 24, 2024 -- Listen Share Key Highlights Generative AI Advancements : Meta’s Llama 3.1 model introduces significant improvements over Llama 3, especially in problem-solving capabilities, context length, and multilingual support. Model Recommendations: Llama 3.1 70B is ideal for long-form content and complex document analysis, while Llama 3 70B is better for real-time interactions. LLM API Flexibility : The LLM API allows developers to seamlessly switch between models, facilitating direct comparisons and maximizing each model’s strengths. Getting Started : A step-by-step guide is provided for integrating Llama models through the Novita AI LLM API, including signing up for access and testing features. Exploration Opportunities : Users can experiment with newer Llama models in the Novita AI LLM Playground ahead of the official Llama 3 API release. Introduction Generative AI is seeing new and creative Llama models. Meta’s newest model, Llama 3.1, shows how far we’ve come. This update improves on Llama 3 and offers big upgrades for many types of problem-solving tasks. In this blog, we will explain the main differences between Llama 3 and Llama 3.1. This will help you choose the best option for your AI needs. Exploring the Evolution from Meta Llama 3 to Llama 3.1 The launch of Llama 3 was an important step for open-source generative AI. Still, Meta saw room for improvements, especially in context length, multilingual support, and safety. These areas were key in creating Llama 3.1. With Llama 3.1, Meta fixes these main problems. It gives developers and researchers better tools to work with. This upgrade offers a big jump in skills, making Llama 3.1 a strong option against top private models. What is Llama 3? Meta has developed and released the Meta Llama 3 family of large language models (LLMs), which includes a collection of pretrained and instruction-tuned generative text models available in 8 billion and 70 billion parameters. The Llama 3 instruction-tuned models are specifically optimized for dialogue applications and consistently outperform many existing open-source chat models on common industry benchmarks. Additionally, we prioritized optimizing for helpfulness and safety during the development of these models. The Llama 3 model is available in two sizes — 8 billion and 70 billion parameters — with both pre-trained and instruction-tuned variants. What is Llama 3.1? The Meta Llama 3.1 collection features multilingual large language models (LLMs) that include pretrained and instruction-tuned generative models in sizes of 8 billion, 70 billion, and 405 billion parameters (text in/text out). The Llama 3.1 instruction-tuned text-only models (8B, 70B, and 405B) are specifically optimized for multilingual dialogue applications and consistently outperform many available open-source and proprietary chat models on common industry benchmarks. Key Differences Between llama 3 vs 3.1 While Llama 3 and Llama 3.1 use the same dense transformer design, there are several important differences between them. One of the biggest differences is their context length. Llama 3.1 has a much larger context window. This lets it handle more text at once. Because of this, it performs better with long documents or complex conversations than Llama 3. Llama 3.1 also has many important updates: Improved Text Generation: The training of Llama 3.1 has been refined. This means it creates text that is more clear, relevant, and sounds more human. Multilingual Skills: Llama 3.1 can work with more languages compared to Llama 3. This makes it useful for a wider range of tasks. Strong Safety Features: Llama 3.1 includes better safety measures. These help to reduce risks linked to troublesome outputs that could arise from the longer context windows. These updates show that Llama 3.1 is a more flexible and powerful tool for developers who need advanced text generation and processing abilities. The Llama models have now been updated to Llama 3.2. If you want to learn more about the differences between Meta Llama 3.2, Llama 3.1, and Llama 3, you can click here to watch a detailed video explanation or simply click the video below. Having explored the key differences between Llama 3 and Llama 3.1, it’s important to turn our attention to a specific comparison: Llama 3 70B versus Llama 3.1 70B. This analysis will showcase their unique features, performance metrics, and practical applications, enabling developers to make informed choices tailored to their needs in dialogue and text generation. Llama 3 70B vs Llama 3.1 70B Choosing between Llama 3 70B and Llama 3.1 70B depends on what your project needs. If you need to handle a lot of context, create long content, or solve complex problems, Llama 3.1 70B is the better option. But, if you care more about speed and efficiency, then Llama 3 70B is still a strong choice. It works well for quick responses and real-time tasks. Basic Comparison Here’s a fundamental comparison between the two models. Benchmark Comparison Llama 3.1 70B outperforms its predecessor in most benchmarks, with significant improvements in: MMLU (+4 points) : This benchmark evaluates performance across 57 subjects in STEM, humanities, social sciences, and more, with questions ranging from elementary to advanced professional levels. It assesses both general knowledge and problem-solving skills. MATH (+17.6 points) : MATH is a new dataset containing 12,500 challenging mathematics problems designed for competitions. GSM8K (+2.1 points) : GSM8K features 8,500 high-quality, linguistically diverse math word problems for grade school students, created by human writers. The dataset is divided into 7,500 training problems and 1,000 test problems. HumanEval (-1.2 points) : This indicates a slight decrease in coding performance. The dataset includes 164 original programming problems that assess language comprehension, algorithms, and basic mathematics, some of which resemble typical software interview questions. Overall, Llama 3.1 70B shows superior performance, especially in mathematical reasoning tasks, while maintaining comparable coding abilities. Speed Comparison The team conducted tests using Keywords AI’s model playground to compare the speed performance of Llama 3 70B and Llama 3.1 70B. Latency The tests, consisting of hundreds of requests for each model, revealed a significant difference in latency. Llama 3 70B demonstrated superior speed with an average latency of 4.75 seconds, while Llama 3.1 70B averaged 13.85 seconds. This nearly threefold difference in response time highlights Llama 3 70B’s advantage in scenarios requiring quick real-time responses, potentially making it a more suitable choice for time-sensitive applications, despite the improvements seen in Llama 3.1 70B in other areas. TTFT (Time to First Token) The tests reveal a significant difference in TTFT performance. Llama 3 70B excels with a TTFT of 0.32 seconds, while Llama 3.1 70B lags behind at 0.60 seconds. This twofold speed advantage for Llama 3 70B could be crucial for applications requiring rapid response initiation, such as voice AI systems, where minimizing perceived delay is essential for user experience. Throughput (Tokens per Second) Llama 3 70B demonstrates significantly higher throughput, processing 114 tokens per second compared to Llama 3.1 70B’s 50 tokens per second. This substantial difference in processing speed — more than double — highlights Llama 3 70B’s superior performance in generating text quickly, making it potentially more suitable for applications that require rapid content generation or real-time interactions. Model Recommendations Both Llama 3 70B and Llama 3.1 70B offer useful features for AI. It is important to know their strengths when choosing the best model for you. Llama 3.1 70B Best for: Long-form content generation, complex document analysis, tasks that require extensive context understanding, advanced logical reasoning, and applications that benefit from larger context windows and output capacities. Not suitable for: Time-sensitive applications requiring rapid responses, real-time interactions where low latency is critical, or projects with limited computational resources that cannot accommodate the model’s increased demands. Llama 3 70B Best for: Applications that require quick response times, real-time interactions, efficient coding tasks, processing shorter documents, and projects where computational efficiency is a priority. Not suitable for: Tasks involving very long documents or complex contextual understanding that exceeds its 8K context window, advanced logical reasoning problems, or applications requiring the processing of extensive contextual information. The general sentiment on Reddit regarding Llama 3 70B vs. Llama 3.1 70B is illustrated in the following image. Llama 3 offers faster response times, while Llama 3.1 excels in tasks needing deeper contextual understanding. The LLM API’s flexibility allows developers to easily switch between the two models without complex configurations, enabling direct comparisons of their performance and features. This helps developers leverage each model’s strengths and make informed decisions, unlocking their potential across various use cases. Getting Started with Llama Models in Novita AI’s LLM API Follow these detailed steps closely to build a powerful language processing application using the Llama model API on Novita AI. This comprehensive guide is tailored to ensure a smooth, efficient development process, meeting the needs of developers seeking advanced AI platforms. Step 1: Sign Up for API Access : Visit the official Novita AI website and create an account. Then, navigate to the API key management section to generate your API key. Step 2: Review the Documentation : Carefully go through the Novita AI API documentation. Step 3: Integrate the Novita LLM API : Enter your API key into Novita AI’s LLM API to start generating concise summaries. Step 4: Test and Add Optional Features : Process the API response and display it in a user-friendly format. Consider adding features like topic extraction or keyword highlighting. Exploring Llama Models in the LLM Playground on Novita AI You can also experiment with Llama’s newer models in the Novita AI LLM Playground before the Llama 3 API is officially released. Step 1: Access the Playground : Navigate to the “ Model API ” tab and select “ LLM Playground ” to start experimenting with the Llama models. Step 2: You can select from the various models in the Llama family within the playground. Step 3: Enter Your Prompt and Generate : Type your desired prompt into the input field provided. This is where you can enter the text or question you want the model to respond to. Conclusion In summary, knowing the differences between Llama 3 and Llama 3.1 can really help you pick the right model for your needs. Llama 3 has its own perks, but Llama 3.1 brings improvements that might fit your needs better. By looking into the key differences and how well each model performs, you can make a smart choice that fits your goals. Whether you care about speed, accuracy, or how they work with Novita AI’s LLM API, choosing the right Llama model is important for boosting your AI abilities. Check out the features, compare the benchmarks, and think about your case to see which version works best for you. Frequently Asked Questions How to access llama 3? Llama 3, an open-source model for the AI community, has a limited context window of 8,192 tokens. This limitation may pose challenges for tasks requiring extensive text data. Is llama 3.1 better than GPT-4? If you prioritize accuracy and efficiency in coding tasks, Llama 3 might be the better choice. Is llama 3.1 restricted? Users must prominently display “Built with Llama” on related websites, interfaces, or documentation. Can Llama 3 run locally? To simplify running Llama 3 on your local machine, use Ollama, an open-source tool. It allows users to run large language models locally and deploy them in Docker containers for easy access. Originally published at Novita AI Novita AI is the All-in-one cloud platform that empowers your AI ambitions. Integrated APIs, serverless, GPU Instance — the cost-effective tools you need. Eliminate infrastructure, start free, and make your AI vision a reality. Llm API Llama 3 Follow Written by Novita AI 150 Followers · 3 Following Deploy AI models effortlessly with our simple API. Build and scale on the most affordable, reliable GPU cloud. Follow No responses yet Help Status About Careers Press Blog Privacy Terms Text to speech Teams\",\n",
      "  \"key_points\": [\n",
      "    \"Llama 3.1 has a larger context window than Llama 3, making it better for long documents or complex conversations.\",\n",
      "    \"Llama 3.1 has improved text generation, multilingual skills, and strong safety features compared to Llama 3.\",\n",
      "    \"Llama 3.1 outperforms Llama 3 in most benchmarks, with significant improvements in MMLU, MATH, GSM8K, and HumanEval.\",\n",
      "    \"Llama 3.1 has a faster response time than Llama 3, with an average latency of 13.85 seconds compared to 4.75 seconds.\",\n",
      "    \"Llama 3.1 has higher throughput than Llama 3, processing 114 tokens per second compared to 50 tokens per second.\"\n",
      "  ],\n",
      "  \"relevance_score\": 9\n",
      "}\n",
      "```\n",
      "  Processing result 4: What Is Meta's Llama 3.3 70B? How It Works, Use Ca...\n",
      "    Warning: File not found - None\n",
      "  Processing result 5: Llama models | Generative AI...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "  \"main_content\": \"Llama models | Generative AI | Google Cloud Skip to main content / English Deutsch Español – América Latina Français Indonesia Italiano Português – Brasil 中文 – 简体 日本語 한국어 Sign in Generative AI Contact Us Start free Home Generative AI Documentation Send feedback Llama models Stay organized with collections Save and categorize content based on your preferences. Llama models on Vertex AI offer fully managed and serverless models as APIs. To use a Llama model on Vertex AI, send a request directly to the Vertex AI API endpoint. Because Llama models use a managed API, there's no need to provision or manage infrastructure. You can stream your responses to reduce the end-user latency perception. A streamed response uses server-sent events (SSE) to incrementally stream the response. Available Llama models The following Llama models are available from Meta to use in Vertex AI. To access a Llama model, go to its Model Garden model card. Llama 3.3 Llama 3.3 is a text-only 70B instruction-tuned model that provides enhanced performance relative to Llama 3.1 70B and to Llama 3.2 90B when used for text-only applications. Go to the Llama 3.3 model card During the Preview period, you are charged as you use the model (pay as you go). For pay-as-you-go pricing, see Llama model pricing on the Vertex AI pricing page. If you require a production-ready service, use the self-hosted Llama models. Llama 3.2 Llama 3.2 lets developers to build and deploy the latest generative AI models and applications that use the latest Llama's capabilities, such as image reasoning. Llama 3.2 is also designed to be more accessible for on-device applications. Go to the Llama 3.2 model card There are no charges during the Preview period. If you require a production-ready service, use the self-hosted Llama models. Considerations When using llama-3.2-90b-vision-instruct-maas, there are no restriction when you send text-only prompts. However, if you include an image in your prompt, the image must be at beginning of your prompt, and you can include only one image. You cannot, for example, include some text and then an image. Llama 3.1 Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. Llama 3.1 405B is Generally Available. You are charged as you use the model (pay as you go). For pay-as-you-go pricing, see Llama model pricing on the Vertex AI pricing page. The other Llama 3.1 models are in Preview. There are no charges for the Preview models. If you require a production-ready service, use the self-hosted Llama models. Go to the Llama 3.1 model card Use Llama models When you send requests to use Llama's models, use the following model names: For Llama 3.3 70B (Preview), use llama-3.3-70b-instruct-maas. For Llama 3.2 90B (Preview), use llama-3.2-90b-vision-instruct-maas. For Llama 3.1 405B (GA), use llama-3.1-405b-instruct-maas. For Llama 3.1 70B (Preview), use llama-3.1-70b-instruct-maas. For Llama 3.1 8B (Preview), use llama-3.1-8b-instruct-maas. We recommend that you use the model versions that include a suffix that starts with an @ symbol because of the possible differences between model versions. If you don't specify a model version, the latest version is always used, which can inadvertently affect your workflows when a model version changes. Before you begin To use Llama models with Vertex AI, you must perform the following steps. The Vertex AI API ( aiplatform.googleapis.com ) must be enabled to use Vertex AI. If you already have an existing project with the Vertex AI API enabled, you can use that project instead of creating a new project. Make sure you have the required permissions to enable and use partner models. For more information, see Grant the required permissions. Sign in to your Google Cloud account. If you're new to Google Cloud, create an account to evaluate how our products perform in real-world scenarios. New customers also get $300 in free credits to run, test, and deploy workloads. In the Google Cloud console, on the project selector page, select or create a Google Cloud project. Go to project selector Make sure that billing is enabled for your Google Cloud project. Enable the Vertex AI API. Enable the API In the Google Cloud console, on the project selector page, select or create a Google Cloud project. Go to project selector Make sure that billing is enabled for your Google Cloud project. Enable the Vertex AI API. Enable the API Go to one of the following Model Garden model cards, then click enable : Go to the Llama 3.3 model card Go to the Llama 3.2 model card Go to the Llama 3.1 model card Make a streaming call to a Llama model The following sample makes a streaming call to a Llama model. REST After you set up your environment, you can use REST to test a text prompt. The following sample sends a request to the publisher model endpoint. Before using any of the request data, make the following replacements: LOCATION : A region that supports Llama models. MODEL : The model name you want to use. ROLE : The role associated with a message. You can specify a user or an assistant. The first message must use the user role. The models operate with alternating user and assistant turns. If the final message uses the assistant role, then the response content continues immediately from the content in that message. You can use this to constrain part of the model's response. CONTENT : The content, such as text, of the user or assistant message. MAX_OUTPUT_TOKENS : Maximum number of tokens that can be generated in the response. A token is approximately four characters. 100 tokens correspond to roughly 60-80 words. Specify a lower value for shorter responses and a higher value for potentially longer responses. STREAM : A boolean that specifies whether the response is streamed or not. Stream your response to reduce the end-use latency perception. Set to true to stream the response and false to return the response all at once. ENABLE_LLAMA_GUARD : A boolean that specifies whether to enable Llama Guard on your inputs and outputs. By default, Llama Guard is enabled and flags responses if it determines they are unsafe. HTTP method and URL: POST https:// LOCATION -aiplatform.googleapis.com/v1beta1/projects/ PROJECT_ID /locations/ LOCATION /endpoints/openapi/chat/completions Request JSON body: { \"model\": \"meta/ MODEL \", \"messages\": [ { \"role\": \" ROLE \", \"content\": \" CONTENT \" } ], \"max_tokens\": MAX_OUTPUT_TOKENS, \"stream\": true, \"extra_body\": { \"google\": { \"model_safety_settings\": { \"enabled\": ENABLE_LLAMA_GUARD, \"llama_guard_settings\": {} } } } } To send your request, choose one of these options: curl Save the request body in a file named request.json, and execute the following command: curl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https:// LOCATION -aiplatform.googleapis.com/v1beta1/projects/ PROJECT_ID /locations/ LOCATION /endpoints/openapi/chat/completions\" PowerShell Save the request body in a file named request.json, and execute the following command: $cred = gcloud auth print-access-token $headers = @{ \"Authorization\" = \"Bearer $cred\" } Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https:// LOCATION -aiplatform.googleapis.com/v1beta1/projects/ PROJECT_ID /locations/ LOCATION /endpoints/openapi/chat/completions\" | Select-Object -Expand Content You should receive a JSON response similar to the following. Response data: { \"choices\": [ { \"delta\": { \"content\": \" CONTENT \", \"role\": \"assistant\", \"refusal: \" REFUSAL_REASON \" #If using Llama Guard and response was flagged by Llama Guard }, \"index\": 0 } ], \"model\": \"meta/ MODEL_NAME \", \"object\": \"chat.completion.chunk\" } data: { \"choices\": [ { \"delta\": { \"content\": \" CONTENT \", \"role\": \"assistant\" }, \"finish_reason\": \"stop\", \"index\": 0 } ], \"model\": \"meta/ MODEL_NAME \", \"object\": \"chat.completion.chunk\", \"usage\": { \"completion_tokens\": 131, \"prompt_tokens\": 14, \"total_tokens\": 145 } } Make a unary call to a Llama model The following sample makes a unary call to a Llama model. REST After you set up your environment, you can use REST to test a text prompt. The following sample sends a request to the publisher model endpoint. Before using any of the request data, make the following replacements: LOCATION : A region that supports Llama models. MODEL : The model name you want to use. ROLE : The role associated with a message. You can specify a user or an assistant. The first message must use the user role. The models operate with alternating user and assistant turns. If the final message uses the assistant role, then the response content continues immediately from the content in that message. You can use this to constrain part of the model's response. CONTENT : The content, such as text, of the user or assistant message. MAX_OUTPUT_TOKENS : Maximum number of tokens that can be generated in the response. A token is approximately four characters. 100 tokens correspond to roughly 60-80 words. Specify a lower value for shorter responses and a higher value for potentially longer responses. STREAM : A boolean that specifies whether the response is streamed or not. Stream your response to reduce the end-use latency perception. Set to true to stream the response and false to return the response all at once. ENABLE_LLAMA_GUARD : A boolean that specifies whether to enable Llama Guard on your inputs and outputs. By default, Llama Guard is enabled and flags responses if it determines they are unsafe. HTTP method and URL: POST https:// LOCATION -aiplatform.googleapis.com/v1beta1/projects/ PROJECT_ID /locations/ LOCATION /endpoints/openapi/chat/completions Request JSON body: { \"model\": \"meta/ MODEL \", \"messages\": [ { \"role\": \" ROLE \", \"content\": \" CONTENT \" } ], \"max_tokens\": MAX_OUTPUT_TOKENS, \"stream\": false, \"extra_body\": { \"google\": { \"model_safety_settings\": { \"enabled\": ENABLE_LLAMA_GUARD, \"llama_guard_settings\": {} } } } } To send your request, choose one of these options: curl Save the request body in a file named request.json, and execute the following command: curl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https:// LOCATION -aiplatform.googleapis.com/v1beta1/projects/ PROJECT_ID /locations/ LOCATION /endpoints/openapi/chat/completions\" PowerShell Save the request body in a file named request.json, and execute the following command: $cred = gcloud auth print-access-token $headers = @{ \"Authorization\" = \"Bearer $cred\" } Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https:// LOCATION -aiplatform.googleapis.com/v1beta1/projects/ PROJECT_ID /locations/ LOCATION /endpoints/openapi/chat/completions\" | Select-Object -Expand Content You should receive a JSON response similar to the following. Response { \"choices\": [ { \"finish_reason\": \"stop\", \"index\": 0, \"message\": { \"content\": \" CONTENT \", \"role\": \"assistant\", \"refusal: \" REFUSAL_REASON \" #If using Llama Guard and response was flagged by Llama Guard } } ], \"model\": \"meta/llama3-405b-instruct-maas\", \"object\": \"chat.completion\", \"usage\": { \"completion_tokens\": 367, \"prompt_tokens\": 14, \"total_tokens\": 381 } } Flagged responses By default, Llama Guard 3 8B is enabled on all predictions that you make with Llama 3.3 and Llama 3.1 models. By default, Llama Guard 3 11B vision is enabled on all predictions that you make with for Llama 3.2 models. Llama Guard helps safeguard responses by checking inputs and outputs. If Llama Guard determines they are unsafe, it flags the response. If you want to disable Llama Guard, modify the model safety setting. For more information, see the model_safety_settings field in the streaming or unary example. Use Vertex AI Studio For Llama models, you can use Vertex AI Studio to quickly prototype and test generative AI models in the Google Cloud console. As an example, you can use Vertex AI Studio to compare Llama model responses with other supported models such as Google's Gemini. For more information, see Quickstart: Send text prompts to Gemini using Vertex AI Studio. Llama model region availability and quotas For Llama models, a quota applies for each region where the model is available. The quota is specified in queries per minute (QPM). The supported regions, default quotas, and maximum context length for each Llama model is listed in the following tables: Llama 3.3 70B (Preview) Region Quota system Supported context length us-central1 30 QPM 128,000 tokens Llama 3.2 90B (Preview) Region Quota system Supported context length us-central1 30 QPM 128,000 tokens Llama 3.1 405B (GA) Region Quota system Supported context length us-central1 60 QPM 128,000 tokens Llama 3.1 70B (Preview) Region Quota system Supported context length us-central1 60 QPM 128,000 tokens Llama 3.1 8B (Preview) Region Quota system Supported context length us-central1 60 QPM 128,000 tokens If you want to increase any of your quotas for Generative AI on Vertex AI, you can use the Google Cloud console to request a quota increase. To learn more about quotas, see Work with quotas. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-02-24 UTC.\",\n",
      "  \"key_points\": [\n",
      "    \"Llama 3.3 is a text-only 70B instruction-tuned model that provides enhanced performance relative to Llama 3.1 70B and to Llama 3.2 90B when used for text-only applications.\",\n",
      "    \"Llama 3.3 is available from Meta to use in Vertex AI, and it can be used for text-only applications.\",\n",
      "    \"Llama 3.1 405B is Generally Available and can be used for text-only applications.\",\n",
      "    \"Llama models use a managed API, so there's no need to provision or manage infrastructure.\",\n",
      "    \"Llama models can be streamed to reduce the end-user latency perception.\",\n",
      "    \"Llama Guard helps safeguard responses by checking inputs and outputs, and it flags responses if it determines they are unsafe.\",\n",
      "    \"Vertex AI Studio can be used to quickly prototype and test generative AI models in the Google Cloud console.\",\n",
      "    \"Llama model region availability and quotas apply, and quotas are specified in queries per minute (QPM).\",\n",
      "    \"Llama 3.3 70B (Preview) has a quota of 30 QPM and a supported context length of 128,000 tokens.\",\n",
      "    \"Llama 3.2 90B (Preview) has a quota of 30 QPM and a supported context length of 128,000 tokens.\",\n",
      "    \"Llama 3.1 405B (GA) has a quota of 60 QPM and a supported context length of 128,000 tokens.\",\n",
      "    \"Llama 3.1 70B (Preview) has a quota of 60 QPM and a supported context length of 128,000 tokens.\",\n",
      "    \"Llama 3.1 8B (Preview) has a quota of 60 QPM and a supported context length of 128,000 tokens.\"\n",
      "  ],\n",
      "  \"relevance_score\": 8\n",
      "}\n",
      "```\n",
      "\n",
      "Processing results for query: Llama 3.3 new features and improvements\n",
      "  Processing result 1: What is Meta Llama 3.3 70B? Features, Use Cases & ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "  \"main_content\": \"Llama 3.3 is a 70-billion parameter, instruction-tuned AI model optimised for text-based tasks like coding, multilingual tasks, and instruction following. It delivers improved performance compared to Llama 3.1 70B and Llama 3.2 90B in text-based applications. Llama 3.3 features include instruction following, multilingual capabilities, improved code understanding, extended context length, cost-effective performance, synthetic data generation, and more.\",\n",
      "  \"key_points\": [\n",
      "    \"Llama 3.3 excels in interpreting and executing instructions, making it ideal for applications requiring natural language understanding and task completion.\",\n",
      "    \"Llama 3.3 supports multiple languages, ensuring broad usability in diverse linguistic environments, with exceptional performance in tasks requiring multilingual reasoning.\",\n",
      "    \"Llama 3.3 delivers accurate and efficient results for coding tasks, such as code generation and debugging.\",\n",
      "    \"Llama 3.3 offers 405B-level performance at a significantly lower cost, making it an affordable option for developers with budget constraints.\",\n",
      "    \"Llama 3.3 enables efficient synthetic data generation, helping developers address challenges like privacy restrictions and data scarcity.\"\n",
      "  ],\n",
      "  \"relevance_score\": 9\n",
      "}\n",
      "```\n",
      "  Processing result 2: What Is Meta's Llama 3.3 70B? How It Works, Use Ca...\n",
      "    Warning: File not found - None\n",
      "  Processing result 3: Efficient, Accessible Generative AI on CPU with Ne...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "  \"main_content\": \"More Efficient, Accessible Generative AI on CPU with New Llama 3.3 70B Model on Arm Neoverse-powered Google Axion Processors - Arm Newsroom Arm Newsroom Blog Blog December 12, 2024 More Efficient, Accessible Generative AI on CPU with New Llama 3.3 70B Model on Arm Neoverse-powered Google Axion Processors The smaller model size of Llama 3.3 70B makes generative AI processing more accessible to the ecosystem, with fewer computational resources needed. By Na Li, AI Solutions Architect, Arm Share Llama is an open and accessible collection of large language models (LLMs) tailored for developers, researchers, and businesses to innovate, experiment, and responsibly scale their generative AI ideas. The Llama 3.1 405B model stands out as the top-performing model in t he Llama collection. However, deploying and utilizing such a large-scale model presents significant challenges, especially for individuals or organizations lacking extensive computational resources. To address those challenges, Meta is introducing the Llama 3.3 70B model, which retains the same architecture as the Llama 3.1 70B model but incorporates the latest advancements in post-training techniques for greater model evaluation performance, while delivering notable improvements in reasoning, mathematics, general knowledge, instruction following, and tool use. Compared to the Llama 3.1 405B model, it offers similar performance, while being significantly smaller in size. In close partnership with Meta, Arm’s engineering teams evaluated the inferencing performance of the Llama 3.3 70B model on Google Axion, a family of custom Arm64-based processors built on Arm’s Neoverse V2 technology, which are available through the Google Cloud. Google Axion is designed for higher performance, lower power consumption and greater scalability than legacy, off-the-shelf processors, which better prepares its data centers for the age of AI. Our benchmarking shows that C4A virtual machines (VMs) based on Axion processors deliver seamless AI-based experiences when running Llama 3.3 70B model and achieve human readability levels across multiple user batch sizes. Human readability refers to the average speed at which a human can read text. This provides developers with flexibility to attain high-quality performance in text-based applications that is comparable to results produced with Llama 3.1 405B model, while no longer requiring large computational resources. CPU inferencing performance with Llama 3.3 70B on Google Axion processors Google Cloud offers Axion-based C4A VMs with up to 72 vCPUs and 576 GB of RAM. For these tests, we have used mid-range cost-effective c4a-standard-32 machine type to deploy the Llama 3.3 70B model with 4-bit quantization. For running our performance testing, we utilized the popular Llama.cpp framework, which as of version b4265 has been optimized with Arm Kleidi. The Kleidi integration provides optimized kernels to ensure AI frameworks can by default unlock the AI capabilities and performance of Arm CPUs. Now let’s get a closer look at the results. Prompt encoding speed refers to how quickly user inputs are processed and interpreted by the language model. As prompt encoding is done in parallel and leverages multiple cores, as shown in Figure 1, performance remains consistent at around ~50 tokens per second across various batch sizes, and the speed is comparable for the different prompt sizes tested. Figure 1: Prompt Encoding Speed with the Llama 3.3 70B model. The prompt encoding speed remains consistent around ~50 tokens per second across various batch sizes, and the speed is comparable for the different prompt sizes tested. Token generation speed measures the rate at which the model generates responses when running Llama 3.3 70B model. Arm Neoverse CPUs optimize machine learning workflows with advanced SIMD instructions, such as Neon and SVE, that are designed to accelerate General Matrix Multiplication (GEMM). To further boost throughput, especially for larger batch sizes, Arm has introduced specialized optimization instructions like SDOT (Signed Dot Product) and MMLA (Matrix Multiply Accumulate). As shown in Figure 2, the token generation speed increases with larger user batch sizes, while remaining relatively consistent across different token generation sizes tested. This capability to achieve higher throughput with larger batch sizes is essential for building scalable systems capable of serving multiple users effectively. Figure 2: Prompt Generation Speed Using the Llama 3.3 70B model. The token generation speed increases with larger user batch sizes, while remaining relatively consistent across different token generation sizes tested. To evaluate the performance perceived by each user when multiple users are interacting with the model at the same time, we measured the token generation speed per batch. Token generation speed per batch is critical, as it directly influences the real-time experience during user interactions with the model. As shown in Figure 3, the token generation speed reached an average human readability level for batch sizes up to 4, indicating that the performance remains stable as the system scales to accommodate multiple users. To accommodate larger numbers of concurrent users, leveraging serving frameworks like vLLM is beneficial, as these frameworks optimize KV cache management to enhance scalability. Figure 3: Comparison of prompt generation speed per user running the Llama 3.3 70B model in batch mode and the average human readability level across various batch sizes. A game-changer for generative AI The new Llama 3.3 70B model is a potential game-changer in the accessibility and efficiency for utilizing the benefits of large-scale AI. The smaller model size makes generative AI processing more accessible to the ecosystem, with large computational resources no longer required. Meanwhile, the Llama 3.3 70B model helps to deliver more efficient AI processing that is vital for datacenter and cloud workloads, while delivering comparable performance to Llama 3.1 405B model in terms of model evaluation benchmark. Through our benchmarking work, we have demonstrated how Google Axion processors, powered by Arm Neoverse, provide a smooth and efficient experience when running the Llama 3.3 70B model, delivering text generation with human-level readability across multiple user batch sizes tested. We’re proud to continue our close partnership with Meta to enable open-source AI innovation on the Arm compute platform, helping to ensure that Llama LLMs operate seamlessly and efficiently across hardware platforms. This blog also had contributions from Milos Puzovic, Technical Director, Arm, and Nobel Chowdary Mandepudi, Graduate Software Engineer, Arm. The Arm Meta partnership Learn more about how Arm and Meta are unlocking AI technologies together. Read Now By Na Li, AI Solutions Architect, Arm Share Article Text Copy Text Any re-use permitted for informational and non-commercial or personal use only. Editorial Contact Arm Editorial Team editorial@arm.com Subscribe to Blogs and Podcasts Get the latest blogs & podcasts direct from Arm\",\n",
      "  \"key_points\": [\n",
      "    \"The new Llama 3.3 70B model is a potential game-changer in the accessibility and efficiency for utilizing the benefits of large-scale AI.\",\n",
      "    \"The smaller model size makes generative AI processing more accessible to the ecosystem, with large computational resources no longer required.\",\n",
      "    \"The Llama 3.3 70B model helps to deliver more efficient AI processing that is vital for datacenter and cloud workloads, while delivering comparable performance to Llama 3.1 405B model in terms of model evaluation benchmark.\",\n",
      "    \"Google Axion processors, powered by Arm Neoverse, provide a smooth and efficient experience when running the Llama 3.3 70B model, delivering text generation with human-level readability across multiple user batch sizes tested.\",\n",
      "    \"The Llama 3.3 70B model retains the same architecture as the Llama 3.1 70B model but incorporates the latest advancements in post-training techniques for greater model evaluation performance.\"\n",
      "  ],\n",
      "  \"relevance_score\": 9\n",
      "}\n",
      "```\n",
      "  Processing result 4: Meta Releases Llama 3.3: a Model with Enhanced Per...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "  \"main_content\": \"Meta Releases Llama 3.3: a Model with Enhanced Performance\\n\\nMeta's latest innovation, Llama 3.3, sets a new benchmark in multilingual AI capabilities, offering unparalleled performance and efficiency. Unveiling Llama 3.3: Meta's Multilingual Marvel\\n\\nMeta has introduced Llama 3.3, a groundbreaking multilingual model in its Llama series, aimed at improving AI research and industry applications. This next-generation language model boasts cutting-edge features designed for enhanced efficiency, scalability, and performance. With its focus on responsible deployment, Llama 3.3 addresses critical risk areas in AI while opening new frontiers in global operations and multilingual capabilities.\\n\\nKey Features of Llama 3.3\\n\\n128k-token Context Window\\n\\nThis expansive context window supports in-depth dialogues, complex instructions, and sophisticated reasoning tasks over extended inputs, significantly outperforming other chat models.\\n\\nArchitectural Advancements\\n\\nInnovations in model architecture improve efficiency in handling a diverse range of applications, from coding tasks to safety evaluations.\\n\\nTraining Data and Responsible Deployment\\n\\nTrained on an enriched dataset emphasising instruction-following and multilingual diversity, Llama 3.3 prioritises responsible AI use to mitigate risks in areas such as bias and misuse.\\n\\nOpen Source Availability\\n\\nUnder a community license, Llama 3.3 is freely accessible on Hugging Face, empowering developers worldwide to explore its potential in various domains, from research to industrial applications.\\n\\nApplications and Use Cases\\n\\nMultilingual Dialogue : Facilitates seamless communication across languages, fostering inclusivity in AI-driven solutions.\\n\\nCoding and Development : Supports advanced code generation, debugging, and optimisation.\\n\\nSafety and Ethical AI : Conducts thorough safety evaluations, ensuring adherence to ethical standards in global deployments.\\n\\nMeta’s Vision for Llama Models\\n\\nWith the Llama series, Meta aims to set the benchmark for next-generation AI models, advancing global operations and critical AI systems.\\n\\nEach model version—from Llama 1 to the latest Llama 3.3—reflects a commitment to innovation, collaboration, and responsible AI development.\\n\\nLlama 3.3 solidifies Meta's leadership in AI by combining cutting-edge technology with a vision for responsible deployment and inclusivity.\\n\\nAs the world adopts open-source AI, this multilingual marvel is poised to transform industries and redefine how humans interact with AI-driven solutions.\\n\\nWhether you're a developer, a researcher, or a business innovator, Llama 3.3 offers the tools to unlock new possibilities in the ever-evolving landscape of AI models.\\n\\nAdvanced Capabilities: What Sets Llama 3.3 Apart\\n\\nLlama 3.3 redefines the landscape of large language models with advanced capabilities that cater to the growing demands of AI applications across industries.\\n\\nA s a next-generation multilingual model, it introduces significant innovations in both performance and usability, cementing its position as a leader among generative models.\\n\\n1. Longer Context Window\\n\\nOne of the standout features of Llama 3.3 is its extended context window of up to 128k tokens, which is far superior to that of previous models.\\n\\nThis advancement: Enables in-depth analysis of longer documents and more complex natural language generation tasks.\\n\\nFacilitates seamless handling of extended multilingual dialogues, instruction-heavy interactions, and comprehensive reasoning.\\n\\nThis capability makes Llama 3.3 an ideal choice for tasks like: Research analysis requiring detailed input-output relationships.\\n\\nGlobal operations demanding high-quality translations and communication in diverse languages.\\n\\n3. Fine-Tuned with Human Feedback\\n\\nThe fine-tuning process of Llama 3.3 incorporates: Supervised Learning : Ensures foundational accuracy for complex language tasks.\\n\\nReinforcement Learning from Human Feedback (RLHF) : Focuses on improving model safety, helpfulness, and reliability.\\n\\nThis meticulous process ensures Llama 3.3 excels in AI applications like: Assisting in instruction-heavy workflows, such as creating multilingual training manuals.\\n\\nImproving model safety by addressing potential risks like bias and misinformation.\\n\\n4. Llama Guard and Safety Measures\\n\\nMeta has equipped Llama 3.3 with Llama Guard, a robust safety framework that: Monitors and mitigates critical risks during deployment.\\n\\nEnsures responsible usage in line with model developer guidelines.\\n\\nEnhances ethical compliance across AI applications in sectors like healthcare, education, and business.\\n\\n6. Comparison to Previous and Future Models\\n\\nCompared to previous models, Llama 3.3 offers: Greater flexibility with extended context and improved architecture.\\n\\nStronger safety protocols for responsible AI deployment.\\n\\nLooking ahead, Meta’s effort for future versions of the Llama series is expected to build on the strengths of Llama 3.3 model.\\n\\nPushing the boundaries of multilingual AI and generative capabilities.\\n\\nCollaboration with Microsoft\\n\\nMeta’s collaboration with Microsoft amplifies Llama 3.3’s potential, leveraging Azure's cloud infrastructure for seamless scalability and availability.\\n\\nThis partnership ensures the model is accessible to researchers and businesses via platforms like Hugging Face.\\n\\nBenchmarking Success: Performance Metrics of Llama 3.3\\n\\nLlama 3.3, Meta’s next-generation multilingual model, delivers industry-leading results across key benchmarks, solidifying its status as a powerhouse in artificial intelligence.\\n\\nWith its 70-billion-parameter architecture, it sets a high bar for both open-source and proprietary AI models, excelling in areas like reasoning, code generation, and multilingual proficiency.\\n\\nAdvancements in Llama Systems and Static Models\\n\\nLlama 3.3 operates seamlessly within the broader Llama systems framework, leveraging static model optimisation to improve both speed and efficiency.\\n\\nKey benefits include: Streamlined performance across supported platforms.\\n\\nBetter resource allocation, making it suitable for both enterprise and research applications.\\n\\nSupported Languages and Applications\\n\\nLlama 3.3 multilingual model is designed to support a wide range of languages, ensuring accessibility and usability across global regions.\\n\\nIts applications span: Multilingual chatbots and AI assistants.\\n\\nInstructional content generation for educational purposes.\\n\\nCompliance with acceptable use policies, prioritizing ethical deployment.\\n\\nComparative Performance Against Other Models\\n\\nWhen benchmarked against other models, Llama 3.3 demonstrates clear superiority: Outperforming previous versions in reasoning, coding, and multilingual tasks.\\n\\nCompeting effectively with proprietary AI systems while maintaining its open-source advantage.\\n\\nReal-World Applications: Leveraging Llama 3.3 in Consumer Goods\\n\\nLlama 3.3, Meta’s next-generation multilingual model, is revolutionising the consumer goods industry with its advanced text-generation capabilities and multilingual fluency.\\n\\nDesigned for versatility and efficiency, the model offers numerous opportunities to enhance customer engagement, improve operational workflows, and streamline content creation.\\n\\nReal-World Impact and Use Cases\\n\\nLlama 3.3 capabilities have transformative implications for the consumer goods sector: E-commerce : Enhancing online shopping experiences with real-time chatbots and personalized product recommendations.\\n\\nRetail Operations : Supporting inventory management and predictive analytics for demand forecasting.\\n\\nBrand Communication : Generating multilingual content for global advertising campaigns.\\n\\nPrioritising Safety: Meta's Commitment to Secure AI Safety is a cornerstone of Meta’s approach to developing next-generation models, and Llama 3.3 exemplifies this focus.\\n\\nThe model integrates advanced safeguards, ensuring that it can be deployed responsibly across a wide range of applications while minimizing potential risks.\\n\\nThe Role of Safety in Next-Generation Models\\n\\nAs a next-generation multilingual model, Llama 3.3 represents a significant advancement in balancing innovation with responsibility.\\n\\nIts safety-first design ensures that the model can be deployed across diverse domains without compromising ethical standards or user trust.\\n\\nLlama 3.3 enables the development of secure AI systems for a variety of use cases: Finance: Safeguarding sensitive financial data in customer interactions.\\n\\nLegal: Ensuring compliance with data protection regulations in document generation.\\n\\nE-commerce: Providing secure recommendations and protecting user privacy.\\n\\nReleased under an open-source community license, Llama 3.3 provides developers with access to cutting-edge safety features.\\n\\nMeta’s comprehensive documentation and support for tools like Prompt Guard and Code Shield ensure seamless implementation of robust safeguards.\\n\\nLlama 3.3 sets a new benchmark for secure AI deployment, reflecting Meta’s unwavering commitment to prioritizing safety alongside innovation.\\n\\nIts advanced safety features, combined with Meta’s developer guidance, make it one of the most reliable next-generation models available.\\n\\nBy incorporating tools like Prompt Guard, developers can harness the full potential of Llama 3.3 capabilities while adhering to the highest standards of ethical AI deployment.\\n\\nmeta, meta quest 3\\n\\nRelated posts\\n\\nmeta for work, Mesh, meta\\n\\nExploring the Open-Source Potential of Meta Llama Erin Finister Sep 24, 2024 2:08:09 PM\\n\\nDiscover the revolutionary open-source capabilities of Meta Llama, a generative AI model that...\\n\\nRead more\\n\\nAI search tool, meta, smart glasses and AI\\n\\nNavigating the Challenges of Open-Sourcing Meta's AI Erin Finister Dec 19, 2024 2:38:34 PM\\n\\nMeta's bold step to release Llama 2 as an open-source AI model has sparked both innovation and...\\n\\nRead more\\n\\nAI search tool, meta, Revolutionising Industries\\n\\nInstalling Llama 3.2 on Your Phone: A Step-by-Step Guide Erin Finister Oct 10, 2024 3:00:00 PM\\n\\nUnlock the full potential of Meta's latest AI model, Llama 3.2, by installing it on your...\\n\\nRead more\",\n",
      "  \"key_points\": [\n",
      "    \"Llama 3.3 offers unparalleled performance and efficiency in multilingual AI capabilities\",\n",
      "    \"Llama 3.3 boasts cutting-edge features designed for enhanced efficiency, scalability, and performance\",\n",
      "    \"Llama 3.3 is a groundbreaking multilingual model in its Llama series, aimed at improving AI research and industry applications\",\n",
      "    \"Llama 3.3 has an extended context window of up to 128k tokens, supporting in-depth dialogues, complex instructions, and sophisticated reasoning tasks\",\n",
      "    \"Llama 3.3 is fine-tuned with human feedback, incorporating supervised learning and reinforcement learning from human feedback\",\n",
      "    \"Llama 3.3 has a robust safety framework, Llama Guard, that monitors and mitigates critical risks during deployment\",\n",
      "    \"Llama 3.3 is designed to support a wide range of languages, ensuring accessibility and usability across global regions\",\n",
      "    \"Llama 3.3 has transformative implications for the consumer goods sector, enhancing customer engagement, improving operational workflows, and streamlining content creation\"\n",
      "  ],\n",
      "  \"relevance_score\": 9\n",
      "}\n",
      "```\n",
      "\n",
      "Processing results for query: Cost of running Llama 3.3 on cloud vs local\n",
      "  Processing result 1: Costs to run Llama 3.3 on cloud? : r/LocalLLaMA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "  \"main_content\": \"Costs to run Llama 3.3 on cloud? Question | Help I'm just exploring an idea to have llama 3.3 run a vtuber streaming chat. But trying to understand the costs with hosting it on the cloud (and where?). And if llama 3.3 can be set up with special instructions in the same way a custom GPT could? Like, let's say the llama 3.3 was chatting non stop for 3 hours? How much would that cost? I understand it's cheaper than GPT4o, but I don't understand how that translates to the actual hosting price. Or perhaps there is an easier way to get this end effect? Read more\",\n",
      "  \"key_points\": [\n",
      "    \"Cost of running Llama 3.3 on cloud vs local infrastructure is unknown\",\n",
      "    \"Llama 3.3 can be set up with special instructions for custom use\",\n",
      "    \"Estimated cost for 3 hours of non-stop chat is unknown\"\n",
      "  ],\n",
      "  \"relevance_score\": 6\n",
      "}\n",
      "```\n",
      "  Processing result 2: What Is Meta's Llama 3.3 70B? How It Works, Use Ca...\n",
      "    Warning: File not found - None\n",
      "  Processing result 3: Llama models | Generative AI...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "  \"main_content\": \"Llama models | Generative AI | Google Cloud Skip to main content / English Deutsch Español – América Latina Français Indonesia Italiano Português – Brasil 中文 – 简体 日本語 한국어 Sign in Generative AI Contact Us Start free Home Generative AI Documentation Send feedback Llama models Stay organized with collections Save and categorize content based on your preferences. Llama models on Vertex AI offer fully managed and serverless models as APIs. To use a Llama model on Vertex AI, send a request directly to the Vertex AI API endpoint. Because Llama models use a managed API, there's no need to provision or manage infrastructure. You can stream your responses to reduce the end-user latency perception. A streamed response uses server-sent events (SSE) to incrementally stream the response. Available Llama models The following Llama models are available from Meta to use in Vertex AI. To access a Llama model, go to its Model Garden model card. Llama 3.3 Llama 3.3 is a text-only 70B instruction-tuned model that provides enhanced performance relative to Llama 3.1 70B and to Llama 3.2 90B when used for text-only applications. Go to the Llama 3.3 model card During the Preview period, you are charged as you use the model (pay as you go). For pay-as-you-go pricing, see Llama model pricing on the Vertex AI pricing page. If you require a production-ready service, use the self-hosted Llama models. Llama 3.2 Llama 3.2 lets developers to build and deploy the latest generative AI models and applications that use the latest Llama's capabilities, such as image reasoning. Llama 3.2 is also designed to be more accessible for on-device applications. Go to the Llama 3.2 model card There are no charges during the Preview period. If you require a production-ready service, use the self-hosted Llama models. Considerations When using llama-3.2-90b-vision-instruct-maas, there are no restriction when you send text-only prompts. However, if you include an image in your prompt, the image must be at beginning of your prompt, and you can include only one image. You cannot, for example, include some text and then an image. Llama 3.1 Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. Llama 3.1 405B is Generally Available. You are charged as you use the model (pay as you go). For pay-as-you-go pricing, see Llama model pricing on the Vertex AI pricing page. The other Llama 3.1 models are in Preview. There are no charges for the Preview models. If you require a production-ready service, use the self-hosted Llama models. Go to the Llama 3.1 model card Use Llama models When you send requests to use Llama's models, use the following model names: For Llama 3.3 70B (Preview), use llama-3.3-70b-instruct-maas. For Llama 3.2 90B (Preview), use llama-3.2-90b-vision-instruct-maas. For Llama 3.1 405B (GA), use llama-3.1-405b-instruct-maas. For Llama 3.1 70B (Preview), use llama-3.1-70b-instruct-maas. For Llama 3.1 8B (Preview), use llama-3.1-8b-instruct-maas. We recommend that you use the model versions that include a suffix that starts with an @ symbol because of the possible differences between model versions. If you don't specify a model version, the latest version is always used, which can inadvertently affect your workflows when a model version changes. Before you begin To use Llama models with Vertex AI, you must perform the following steps. The Vertex AI API ( aiplatform.googleapis.com ) must be enabled to use Vertex AI. If you already have an existing project with the Vertex AI API enabled, you can use that project instead of creating a new project. Make sure you have the required permissions to enable and use partner models. For more information, see Grant the required permissions. Sign in to your Google Cloud account. If you're new to Google Cloud, create an account to evaluate how our products perform in real-world scenarios. New customers also get $300 in free credits to run, test, and deploy workloads. In the Google Cloud console, on the project selector page, select or create a Google Cloud project. Go to project selector Make sure that billing is enabled for your Google Cloud project. Enable the Vertex AI API. Enable the API In the Google Cloud console, on the project selector page, select or create a Google Cloud project. Go to project selector Make sure that billing is enabled for your Google Cloud project. Enable the Vertex AI API. Enable the API Go to one of the following Model Garden model cards, then click enable : Go to the Llama 3.3 model card Go to the Llama 3.2 model card Go to the Llama 3.1 model card Make a streaming call to a Llama model The following sample makes a streaming call to a Llama model. REST After you set up your environment, you can use REST to test a text prompt. The following sample sends a request to the publisher model endpoint. Before using any of the request data, make the following replacements: LOCATION : A region that supports Llama models. MODEL : The model name you want to use. ROLE : The role associated with a message. You can specify a user or an assistant. The first message must use the user role. The models operate with alternating user and assistant turns. If the final message uses the assistant role, then the response content continues immediately from the content in that message. You can use this to constrain part of the model's response. CONTENT : The content, such as text, of the user or assistant message. MAX_OUTPUT_TOKENS : Maximum number of tokens that can be generated in the response. A token is approximately four characters. 100 tokens correspond to roughly 60-80 words. Specify a lower value for shorter responses and a higher value for potentially longer responses. STREAM : A boolean that specifies whether the response is streamed or not. Stream your response to reduce the end-use latency perception. Set to true to stream the response and false to return the response all at once. ENABLE_LLAMA_GUARD : A boolean that specifies whether to enable Llama Guard on your inputs and outputs. By default, Llama Guard is enabled and flags responses if it determines they are unsafe. HTTP method and URL: POST https:// LOCATION -aiplatform.googleapis.com/v1beta1/projects/ PROJECT_ID /locations/ LOCATION /endpoints/openapi/chat/completions Request JSON body: { \"model\": \"meta/ MODEL \", \"messages\": [ { \"role\": \" ROLE \", \"content\": \" CONTENT \" } ], \"max_tokens\": MAX_OUTPUT_TOKENS, \"stream\": true, \"extra_body\": { \"google\": { \"model_safety_settings\": { \"enabled\": ENABLE_LLAMA_GUARD, \"llama_guard_settings\": {} } } } } To send your request, choose one of these options: curl Save the request body in a file named request.json, and execute the following command: curl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https:// LOCATION -aiplatform.googleapis.com/v1beta1/projects/ PROJECT_ID /locations/ LOCATION /endpoints/openapi/chat/completions\" PowerShell Save the request body in a file named request.json, and execute the following command: $cred = gcloud auth print-access-token $headers = @{ \"Authorization\" = \"Bearer $cred\" } Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https:// LOCATION -aiplatform.googleapis.com/v1beta1/projects/ PROJECT_ID /locations/ LOCATION /endpoints/openapi/chat/completions\" | Select-Object -Expand Content You should receive a JSON response similar to the following. Response data: { \"choices\": [ { \"delta\": { \"content\": \" CONTENT \", \"role\": \"assistant\", \"refusal: \" REFUSAL_REASON \" #If using Llama Guard and response was flagged by Llama Guard }, \"index\": 0 } ], \"model\": \"meta/ MODEL_NAME \", \"object\": \"chat.completion.chunk\" } data: { \"choices\": [ { \"delta\": { \"content\": \" CONTENT \", \"role\": \"assistant\" }, \"finish_reason\": \"stop\", \"index\": 0 } ], \"model\": \"meta/ MODEL_NAME \", \"object\": \"chat.completion.chunk\", \"usage\": { \"completion_tokens\": 131, \"prompt_tokens\": 14, \"total_tokens\": 145 } } Make a unary call to a Llama model The following sample makes a unary call to a Llama model. REST After you set up your environment, you can use REST to test a text prompt. The following sample sends a request to the publisher model endpoint. Before using any of the request data, make the following replacements: LOCATION : A region that supports Llama models. MODEL : The model name you want to use. ROLE : The role associated with a message. You can specify a user or an assistant. The first message must use the user role. The models operate with alternating user and assistant turns. If the final message uses the assistant role, then the response content continues immediately from the content in that message. You can use this to constrain part of the model's response. CONTENT : The content, such as text, of the user or assistant message. MAX_OUTPUT_TOKENS : Maximum number of tokens that can be generated in the response. A token is approximately four characters. 100 tokens correspond to roughly 60-80 words. Specify a lower value for shorter responses and a higher value for potentially longer responses. STREAM : A boolean that specifies whether the response is streamed or not. Stream your response to reduce the end-use latency perception. Set to true to stream the response and false to return the response all at once. ENABLE_LLAMA_GUARD : A boolean that specifies whether to enable Llama Guard on your inputs and outputs. By default, Llama Guard is enabled and flags responses if it determines they are unsafe. HTTP method and URL: POST https:// LOCATION -aiplatform.googleapis.com/v1beta1/projects/ PROJECT_ID /locations/ LOCATION /endpoints/openapi/chat/completions Request JSON body: { \"model\": \"meta/ MODEL \", \"messages\": [ { \"role\": \" ROLE \", \"content\": \" CONTENT \" } ], \"max_tokens\": MAX_OUTPUT_TOKENS, \"stream\": false, \"extra_body\": { \"google\": { \"model_safety_settings\": { \"enabled\": ENABLE_LLAMA_GUARD, \"llama_guard_settings\": {} } } } } To send your request, choose one of these options: curl Save the request body in a file named request.json, and execute the following command: curl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https:// LOCATION -aiplatform.googleapis.com/v1beta1/projects/ PROJECT_ID /locations/ LOCATION /endpoints/openapi/chat/completions\" PowerShell Save the request body in a file named request.json, and execute the following command: $cred = gcloud auth print-access-token $headers = @{ \"Authorization\" = \"Bearer $cred\" } Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https:// LOCATION -aiplatform.googleapis.com/v1beta1/projects/ PROJECT_ID /locations/ LOCATION /endpoints/openapi/chat/completions\" | Select-Object -Expand Content You should receive a JSON response similar to the following. Response { \"choices\": [ { \"finish_reason\": \"stop\", \"index\": 0, \"message\": { \"content\": \" CONTENT \", \"role\": \"assistant\", \"refusal: \" REFUSAL_REASON \" #If using Llama Guard and response was flagged by Llama Guard } } ], \"model\": \"meta/llama3-405b-instruct-maas\", \"object\": \"chat.completion\", \"usage\": { \"completion_tokens\": 367, \"prompt_tokens\": 14, \"total_tokens\": 381 } } Flagged responses By default, Llama Guard 3 8B is enabled on all predictions that you make with Llama 3.3 and Llama 3.1 models. By default, Llama Guard 3 11B vision is enabled on all predictions that you make with for Llama 3.2 models. Llama Guard helps safeguard responses by checking inputs and outputs. If Llama Guard determines they are unsafe, it flags the response. If you want to disable Llama Guard, modify the model safety setting. For more information, see the model_safety_settings field in the streaming or unary example. Use Vertex AI Studio For Llama models, you can use Vertex AI Studio to quickly prototype and test generative AI models in the Google Cloud console. As an example, you can use Vertex AI Studio to compare Llama model responses with other supported models such as Google's Gemini. For more information, see Quickstart: Send text prompts to Gemini using Vertex AI Studio. Llama model region availability and quotas For Llama models, a quota applies for each region where the model is available. The quota is specified in queries per minute (QPM). The supported regions, default quotas, and maximum context length for each Llama model is listed in the following tables: Llama 3.3 70B (Preview) Region Quota system Supported context length us-central1 30 QPM 128,000 tokens Llama 3.2 90B (Preview) Region Quota system Supported context length us-central1 30 QPM 128,000 tokens Llama 3.1 405B (GA) Region Quota system Supported context length us-central1 60 QPM 128,000 tokens Llama 3.1 70B (Preview) Region Quota system Supported context length us-central1 60 QPM 128,000 tokens Llama 3.1 8B (Preview) Region Quota system Supported context length us-central1 60 QPM 128,000 tokens If you want to increase any of your quotas for Generative AI on Vertex AI, you can use the Google Cloud console to request a quota increase. To learn more about quotas, see Work with quotas. Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-02-24 UTC.\",\n",
      "  \"key_points\": [\n",
      "    \"Llama models are available on Vertex AI, offering fully managed and serverless models as APIs.\",\n",
      "    \"To use a Llama model, send a request directly to the Vertex AI API endpoint.\",\n",
      "    \"Llama models use a managed API, eliminating the need for infrastructure provisioning or management.\",\n",
      "    \"Streaming responses can reduce end-user latency perception.\",\n",
      "    \"Llama models are available in different regions, with quotas specified in queries per minute (QPM).\",\n",
      "    \"The supported regions, default quotas, and maximum context length for each Llama model is listed in the following tables.\",\n",
      "    \"Llama Guard helps safeguard responses by checking inputs and outputs, and can be disabled by modifying the model safety setting.\",\n",
      "    \"Vertex AI Studio can be used to quickly prototype and test generative AI models in the Google Cloud console.\",\n",
      "    \"Llama models can be compared with other supported models, such as Google's Gemini, using Vertex AI Studio.\"\n",
      "  ],\n",
      "  \"relevance_score\": 8\n",
      "}\n",
      "```\n",
      "  Processing result 4: Meta Llama in the Cloud | Llama Everywhere...\n",
      "    Warning: File not found - None\n",
      "  Processing result 5: Llama 3.3 vs. ChatGPT Pro: Key Considerations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "  \"main_content\": \"Llama 3.3 vs. ChatGPT Pro: Key Considerations\\n\\nLlama 3.3 70B on Valdi GPUs\\n\\nThe foundation of any open-source AI deployment begins with proper hardware configuration. Modern AI models typically require at least 40GB of VRAM for basic operation, with recommended configurations including 70GB or more for optimal performance. Parallel processing capabilities become crucial for inference speed, with NVIDIA GPUs supporting CUDA being the standard choice for many deployments. Prerequisites for Llama 3.3 70B specifically are: A GPU capable of running the target model based on memory requirements; for our example we’re using an NVIDIA A40 48 GB from Valdi’s on-demand inventory Docker At least 70GB of available disk space\\n\\nTo get started, we’ll log into Valdi and spin-up an A40 quickly to do our deployment.\\n\\nExample Valdi GPU.  Launch in 30 seconds.\\n\\nNote: When you launch a GPU - if the node requires specific port mapping, be sure to create an external port mapping/forward for 8080 - the public port will be where you access the web UI.  Some of the providers on Valdi do not require port mapping and will provide access to all host ports configured on the node OS.\\n\\nQuick Start with Llama 3.3 70B\\n\\n (Note: for this example, we are running on a Valdi VM with Ubuntu 22.04)\\n\\n Step 1: Install ollama curl -fsSL https://ollama.com/install.sh | sh Step 2: Pull down Llama 3.3 for ollama ollama fetch llama3.3\\n\\n Step 3: Install and run open-webui (Note: most Valdi VM instances come with docker pre-installed) docker run -d --network=host --gpus=all -v ollama:/root/.ollama -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama\\n\\n The installation process with open-webui provides an easy and functional combo for model deployment.  The model weights will be downloaded from the repositories, though downloading times may vary significantly based on internet connection speed. Configuration files can be adjusted for your specific hardware setup, with particular attention to memory allocation and thread management.  Once the docker container is running, you can access the web interface on port 8080 on the public IP (or the forwarding port you configured when the VM was first deployed). You’ll be asked to create the default admin account, then you’re in.\\n\\n Llama 3.3 running on a Valdi A40 GPU\\n\\n Common troubleshooting often centers around CUDA compatibility issues and memory management. Maintaining detailed logs during installation helps identify and resolve these challenges quickly.  With open-webui you can adjust parameters of the environment like number of threads (num_thread), batch size and more.\\n\\n ChatGPT Pro As a cloud solution, ChatGPT Pro does not require any installation or setup.\\n\\n Features OpenAI recently released their ChatGPT Pro offering which includes higher usage limits and access to a variety of their models, in particular O1 Pro mode - which is supported by more compute behind the scenes.\\n\\n Cost Analysis\\n\\n Llama 3.3. 70 70B on Valdi GPUs Hardware & Maintenance: Initial deployment costs center around GPU selection and provisioning. With many quality GPUs - such as those from us at Valdi - it’s becoming very easy to get your own GPUs and even build out a cluster for the ultimate in flexibility and performance.  Monthly costs typically range from $200-$1500 monthly for a performant GPU and can vary by region and provider.\\n\\n Maintenance costs primarily involve time investment for updates and optimization rather than direct financial outlays. Users: Many users, even possibly your entire team, can use the deployment up to the capacity you’ve selected. The table below is a great example of how accuracy of leading models relates to their approximate pricing.\\n\\n While there are a lot of variables to hosting technology, controlling your own AI not only provides superior data privacy, but has a significant cost savings as you scale usage.  We’ve seen similar in the cloud SaaS space over the past decade and it looks like generative AI is following a similar power-law with general LLM deployments.\\n\\n Comparison courtesy Meta / llama.com.\\n\\n ChatGPT Pro Subscription Instead of hardware and maintenance costs, commercial services often have tiered pricing models based on usage volume. OpenAI’s ChatGPT Pro, for example, was just released at $200 USD/month per user.\\n\\n Users: 1 for $200. Tokens & Features Consider not just the base subscription cost but also per-token charges and any additional fees for features like longer context windows or priority access.\\n\\n While the monthly subscription may seem higher initially, it can go even higher still when the company decides to implement a higher priced tier or new metered charges.\\n\\n While higher packages from providers have higher limits, most providers implement usage limits and constraints. (e.g. max number of requests, token and context window limits)\\n\\n The chart below highlights the trend towards price and performance optimization of new model releases.\\n\\n Pricing Data from artificialanalysis.ai 12/11/24.\\n\\n Performance Comparison\\n\\n Response speeds vary significantly between self-hosted and cloud solutions. Local deployments may offer lower latency for individual requests but can struggle with concurrent loads.\\n\\n Output quality depends heavily on the specific model and use case, with both solutions offering competitive results for most applications.\\n\\n Context window capabilities affect the model’s ability to handle longer conversations or documents, with recent advances expanding these capabilities across both platforms.\\n\\n Using datacenter quality GPUs like the Nvidia H100 and H200 can generate performance exceeding commercial SaaS solutions.\\n\\n The chart below shows the trend for models towards increased speed through the networks even while improving accuracy.\\n\\n Performance Data from artificialanalysis.ai 12/11/24.\\n\\n We chose Llama 3.3 because the Llama team at Meta has shown a focus on improving the price vs. performance ratio of foundation models.\\n\\n As the model-card below shows, 3.3 is improving context, in addition to improving accuracy.\\n\\n Model Card for LLama 3.3 - located at https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md\\n\\n Key Considerations\\n\\n Properties of Llama 3.3. 70B and Open Source LLMs\\n\\n Self-hosted solutions provide complete control over data flow and model behavior.\\n\\n Organizations can modify and fine-tune models for specific use cases, ensuring high performance for their particular needs.\\n\\n Privacy-conscious operations benefit from keeping all data within their infrastructure, meeting stringent compliance requirements more easily.\\n\\n Data security, sovereignty, and privacy is highly regarded as a key aspect of open-source models.\\n\\n ChatGPT and Commercial Solutions\\n\\n Cloud-based commercial solutions eliminate infrastructure management concerns.\\n\\n Regular updates ensure access to the latest improvements without manual intervention.\\n\\n The ability to scale usage up or down based on demand provides flexibility for varying workloads.\\n\\n A big issue with proprietary cloud AI solutions is that your data is controlled by a 3rd-party:  every artifact, query and piece of information you submit is able to be used by the commercial SaaS provider as they see fit.\\n\\n Even with published terms-of-services, those terms are always subject to change.\\n\\n Implementation Tips\\n\\n Successful deployment requires careful attention to resource monitoring and optimization.\\n\\n Implement proper security measures, including API key management for commercial services or network isolation for self-hosted solutions.\\n\\n Consider implementing caching mechanisms to improve response times and reduce resource usage.\\n\\n Document your deployment process thoroughly to facilitate future updates and troubleshooting.\\n\\n Additionally: Use a firewall to lockdown ports on your node.\\n\\n Remove unnecessary User and System accounts Activate encryption-at-rest on your OS drive (note you don’t necessarily want to do this for VRAM / TEE features as it has a performance hit).\\n\\n Use an Inference Engine like vllm.\\n\\n Engines are optimized to load models and orchestrate queries.\\n\\n It also makes it easier to scale usage later on.\\n\\n Conclusion:\\n\\n Which option is better?\\n\\n The choice between Llama 3.3 40GB and ChatGPT Pro (and open-source and commercial AI solutions in general) depends heavily on the cost-performance requirements of your specific use case, technical capabilities, and budget constraints.\\n\\n Organizations with strong technical teams that can handle install, maintenance, optimization of self-hosting may benefit most from open-source deployments, while businesses seeking rapid deployment and minimal customization may prefer commercial solutions instead.\\n\\n If security and privacy are your highest priority, controlling your GPUs is still your only option and the tools make it easier to do every day.\\n\\n If not, consider starting with a hybrid approach, using both solutions to understand their practical implications for your specific needs.\\n\\n Unlock the full potential of Llama 3.3 with access to high-performance GPUs designed for large-scale AI workloads.\\n\\n Scale up or down on demand, pay by the second, and deploy your models effortlessly—no long-term commitments, just peak performance when you need it most.\\n\\n Shop our GPU inventory.\\n\\n Copyright 2024 VALDI. All rights reserved.\\n\\n Legal Terms of Service Privacy Policy Acceptable Use Policy Data Processing Policy Support Documentation Contact Us\",\n",
      "  \"key_points\": [\n",
      "    \"Llama 3.3 requires at least 40GB of VRAM for basic operation, with recommended configurations including 70GB or more for optimal performance.\",\n",
      "    \"ChatGPT Pro does not require any installation or setup, but has tiered pricing models based on usage volume.\",\n",
      "    \"Self-hosted solutions provide complete control over data flow and model behavior, but require technical expertise and infrastructure management.\",\n",
      "    \"Cloud-based commercial solutions eliminate infrastructure management concerns, but may have limitations on data security and sovereignty.\",\n",
      "    \"Using datacenter quality GPUs like the Nvidia H100 and H200 can generate performance exceeding commercial SaaS solutions.\"\n",
      "  ],\n",
      "  \"relevance_score\": 8\n",
      "}\n",
      "```\n",
      "\n",
      "Processing results for query: Llama 3.3 vs Llama 3.1 performance comparison\n",
      "  Processing result 1: Llama 3 vs 3.1 vs 3.2 : r/LocalLLaMA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"main_content\": \"Members Online • Ok_Ostrich_8845 Llama 3 vs 3.1 vs 3.2 Question | Help What can you say about these 3 versions of Llama LLMs? Were they trained around the same time? Or 3.2 and 3.1 were later enhancement from 3?\",\n",
      "  \"key_points\": [\n",
      "    \"Comparison between Llama 3.3 and Llama 3.1\",\n",
      "    \"Performance differences between Llama 3.3 and Llama 3.1\",\n",
      "    \"Training time and enhancements of Llama 3.2, 3.1, and 3\"\n",
      "  ],\n",
      "  \"relevance_score\": 8\n",
      "}\n",
      "  Processing result 2: Llama 3.3 70B Instruct vs Llama 3.1 405B Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "  \"main_content\": \"Llama 3.3 70B Instruct vs Llama 3.1 405B Instruct Get a detailed comparison of AI language models Meta's Llama 3.3 70B Instruct and Meta's Llama 3.1 405B Instruct, including model features, token pricing, API costs, performance benchmarks, and real-world capabilities to help you choose the right LLM for your needs.\",\n",
      "  \"key_points\": [\n",
      "    \"Llama 3.3 70B Instruct is a multilingual, instruction-tuned large language model optimized for dialogue use cases.\",\n",
      "    \"Llama 3.1 405B Instruct is a model developed by Meta that supports an input context window of 128K tokens and can generate a maximum of 2,048 tokens per request.\",\n",
      "    \"Llama 3.3 70B Instruct outperforms many open-source and closed chat models across industry benchmarks.\",\n",
      "    \"Llama 3.1 405B Instruct is roughly 5.7x more expensive compared to Llama 3.3 70B Instruct for input and output tokens.\"\n",
      "  ],\n",
      "  \"relevance_score\": 9\n",
      "}\n",
      "```\n",
      "  Processing result 3: Choosing the Best Llama Model: Llama 3 vs 3.1 vs 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "  \"main_content\": \"The Llama model series has been a fascinating journey in the world of AI development. It all started with Meta’s release of the original Llama model, which aimed to democratize access to powerful language models by making them open-source. It allowed researchers and developers to dive deeper into AI without the constraints of closed systems. Fast forward to today, and we have seen significant advancements with the introduction of Llama 3, Llama 3.1, and the latest, Llama 3.2. Each iteration has brought its own unique improvements and capabilities, enhancing the way we interact with AI. For those eager to explore the evolving landscape of AI and its practical applications, our LLM Bootcamp offers hands-on experience with the latest advancements in the field. In this blog, we will delve into a comprehensive comparison of the three iterations of the Llama model: Llama 3, Llama 3.1, and Llama 3.2. We aim to explore their features, performance, and the specific enhancements that each version brings to the table.\",\n",
      "  \"key_points\": [\n",
      "    \"Llama 3 features a transformer-based architecture with parameter sizes of 8 billion and 70 billion, utilizing a standard self-attention mechanism.\",\n",
      "    \"Llama 3.1 is built using an enhanced transformer architecture with parameter sizes of 8 billion, 70 billion, and 405 billion.\",\n",
      "    \"Llama 3.2 is optimized for real-time applications with varying token limits, featuring an integrated multimodal transformer architecture and self-attention.\",\n",
      "    \"Llama 3 can be used for customer service by powering chatbots to handle a wide range of customer inquiries.\",\n",
      "    \"Llama 3.1 is useful for applications like a multilingual customer service agent as it can switch between languages seamlessly and handle intricate technical support queries.\",\n",
      "    \"Llama 3.2 is designed for low latency and efficient performance on mobile and edge devices, making it perfect for real-time, multimodal interactions.\",\n",
      "    \"Llama 3.2 can be used for a mobile app providing real-time language translation with visual inputs.\",\n",
      "    \"Llama 3.2 is useful for authors and scriptwriters to enhance their creative process by offering innovative brainstorming assistance.\",\n",
      "    \"Llama 3.2 can provide assistance for in-depth market research analysis, particularly in understanding customer feedback and social media sentiment.\"\n",
      "  ],\n",
      "  \"relevance_score\": 9\n",
      "}\n",
      "```\n",
      "  Processing result 4: Llama 3.3 just dropped — is it better than GPT-4 o...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "  \"main_content\": \"Llama 3.3 just dropped — is it better than GPT-4 or Claude-Sonnet-3.5? Meta just released their newest AI model Llama 3.3. This 70-billion parameter model caught the attention of the open-source community, showing impressive performance, cost efficiency, and multilingual support while having only ~17% of Llama 3.1 405B's parameters. But is it truly better than the top models in the market? Let’s take a look at how Llama 3.3 70B Instruct compares with previous models and why it's a big deal.\\n\\nComparing Llama 3.3 with Llama 3.1 Faster Inference Speed Llama 3.3 70B is a high-performance replacement for Llama 3.1 70B. Independent benchmarks indicate that Llama 3.3 70B achieves an inference speed of 276 tokens per second on Groq hardware, surpassing Llama 3.1 70B by 25 tokens per second. This makes it a viable option for real-time applications where latency is critical.\\n\\nFewer Parameters, Similar Performance Despite its smaller size, Meta claimed that Llama 3.3 has powerful performance comparable to the much larger Llama 3.1 405B model. With significantly lower computational overhead, developers can deploy it using mid-tier GPUs or run the model locally on their consumer-grade laptops.\\n\\nMultilingual Support for a Global Audience Like its predecessor Llama 3.1, Llama 3.3 also supports 8 languages, including English, Germain, French, Italian, Portuguese, Hindi, Spanish, and Thai. The model is versatile for developers who are targeting global audiences.\\n\\nOn the Multilingual MGSM (0-shot) test, it scored 91.1, which is similar to its predecessor Llama 3.1 70B (91.6) and close to more advanced models like Claude 3.5 Sonnet (92.8).\\n\\nMore cost-effective Llama 3.3 70B has a significant advantage over its costs: $0.10 per million input tokens, compared to $1.00 for Llama 3.1 405B, and $0.40 per million output tokens, compared to $1.80 for Llama 3.1 405B In an AI conversation agent example by Databricks, using Llama 3.3 70B is 88% more cost-effective to deploy than Llama 3.1 405B.\\n\\nCut Llama 3 API costs by up to 70% ⚡️ Use Helicone to cache responses, optimize prompts, and more. Get started for free\\n\\nCalculate API costs\\n\\nExtended context window Llama 3.3 70B supports a large context window of 128,000 tokens like Llama 3.1 405B. This extensive context handling allows both models to process large volumes of data and maintain contextual awareness in conversations.\\n\\nPerformance Benchmarks Llama 3.3 has impressive results across code, math, and multilingual benchmarks. Highlights include: A high score of 92.1 in IFEval (instruction following). 89.0 in HumanEval and 88.6 in MBPP EvalPlus (code).\\n\\nExcels in the Multilingual MGSM benchmark with a score of 91.6. In some evaluations, Llama 3.3 70B even outperforms established models like Google's Gemini 1.5 Pro and OpenAI's GPT-4 on key benchmarks, including MMLU (Massive Multitask Language Understanding).\\n\\nIs Llama 3.3 better than GPT-4 or Claude-Sonnet-3.5? At a glance, Llama 3.3’s open-source nature makes it more customizable and accessible for developers. It also has lower operational costs which appeals to small and mid-sized teams.\\n\\nLlama 3.3 GPT-4 Claude 3 Parameters 70B Unknown (estimated large) ~100B Cost-effectiveness High (low token cost) \\u2022 Moderate Moderate Open Source Yes No No Multilingual Support Moderate Extensive \\u2022 Moderate Fine-Tuning Easy and flexible \\u2022 Limited (API-based) Limited (API-based) Ideal Use Cases Cost-sensitive, domain-specific Broad tasks General NLP tasks\\n\\nHow to access Llama 3.3 70B? Llama 3.3 70B is available through Meta's official Llama site, OpenRouter, Hugging Face, and other AI inferencing platforms. Integrate Llama 3.3 with Helicone \\u2022 Integrate observability with OpenRouter in a few lines of code. See docs for details.\\n\\nfetch (\\\"https://openrouter.helicone.ai/api/v1/chat/completions\\\", { method: \\\"POST\\\", headers: { Authorization: `Bearer ${OPENROUTER_API_KEY} `, \\\"Helicone-Auth\\\": `Bearer ${HELICONE_API_KEY} ` }, body: JSON.stringify({ model: \\\"meta-llama/llama-3.3-70b-instruct\\\", messages: [{ role: \\\"user\\\", content: \\\"What is the meaning of life?\\\" }], stream: true }), });\\n\\nUse Cases of Llama 3.3 Llama 3.3 70B is versatile and can be used for various tasks, including: Chatbots and virtual assistants: Faster model speed and better accuracy helps to improve user experience, especially in customer service applications. Localization and translation services Content creation and summarization: developers report faster output generation for marketing copy, technical writing, and creative projects. Code generation and debugging Synthetic data generation\\n\\nLimitations of Llama 3.3 License restrictions: The license prohibits using any part of the Llama models, including response outputs, to train other AI models. Limited modalities: Llama 3.3 70B is a text-only model, lacking capabilities in other modalities such as image or audio processing Knowledge cutoff: The model's knowledge is limited to information up to December 2023, making it potentially outdated for current events or recent developments79.\\n\\nConclusion Llama 3.3 is a major advancement in open-sourced large language models. The increasing efficiency improvements are allowing developers to access more affordable and incredibly faster models, and more incredibly powerful models that one can run directly on their own device, making it more accessible to the open-source community.\\n\\nLearn about other models: OpenAI Unveils New O3 Model: What Is It and How Is It Different from O1? O1 and ChatGPT Pro — here's everything you need to know GPT-5 — Release date, features & what to expect FAQ 1. How to finetune Llama 3.3? Fine-tuning Llama models can be done in two main ways: Full parameter fine-tuning by adjusting all model parameters. Best performance, but very time-consuming and GPU-intensive. Parameter efficient fine-tuning (PEFT) using either LoRA or QLoRA. Meta’s official fine-tuning guide recommendeds starting with LoRA fine-tuning. If resources are extremely limited, use QLoRA. Then evaluate model performance after fine-tuning, and only consider full parameter fine-tuning if the results are not satisfactory.\\n\\n2. What data was Llama 3.3 70B trained on? Llama 3.3 70B was pretrained on 15 trillion tokens from public sources, 7 times larger than Llama 2’s dataset. The training data includes: New addition of publicly available online data 25+ million synthetically-generated examples for fine-tuning 4x more code data than Llama 2 5%+ non-English data across 30+ languages\\n\\n3. What is the knowledge cutoff of Llama 3.3 70B? Llama 3.3 70B has a knowledge cutoff of December 2023. Questions or feedback? Are the information out of date? Please raise an issue or contact us, we'd love to hear from you!\",\n",
      "  \"key_points\": [\n",
      "    \"Llama 3.3 has a faster inference speed than Llama 3.1, with an average of 276 tokens per second on Groq hardware.\",\n",
      "    \"Llama 3.3 has similar performance to Llama 3.1, despite having only 17% of its parameters.\",\n",
      "    \"Llama 3.3 supports 8 languages, including English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\",\n",
      "    \"Llama 3.3 has a lower computational overhead than Llama 3.1, making it more suitable for deployment on mid-tier GPUs or consumer-grade laptops.\",\n",
      "    \"Llama 3.3 is 88% more cost-effective to deploy than Llama 3.1, with a cost of $0.10 per million input tokens.\"\n",
      "  ],\n",
      "  \"relevance_score\": 9\n",
      "}\n",
      "```\n",
      "  Processing result 5: Llama 3 vs Llama 3.1 : Which is Better for Your AI...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "  \"main_content\": \"Llama 3.1 vs Llama 3.1 performance comparison. Llama 3.1 405B outperforms GPT-4 and Claude 3 Opus in most benchmarks, making it the most powerful open-source model available. However, it may not be the optimal choice for many real-world applications due to its slow generation time and high Time to First Token (TTFT). Llama 3.1 70B emerges as a more practical alternative for developers looking to integrate these models into production or self-host them. Llama 3.1 70B outperforms Llama 3 70B in most benchmarks, particularly in mathematical reasoning. Speed Trade-Off: Llama 3 70B is significantly faster, with lower latency and quicker token generation.\",\n",
      "  \"key_points\": [\n",
      "    \"Llama 3.1 405B outperforms GPT-4 and Claude 3 Opus in most benchmarks\",\n",
      "    \"Llama 3.1 405B has slow generation time and high TTFT\",\n",
      "    \"Llama 3.1 70B is a more practical alternative for production and self-hosting\",\n",
      "    \"Llama 3.1 70B outperforms Llama 3 70B in most benchmarks\",\n",
      "    \"Llama 3 70B is faster than Llama 3.1 70B with lower latency and quicker token generation\"\n",
      "  ],\n",
      "  \"relevance_score\": 8\n",
      "}\n",
      "```\n",
      "\n",
      "Processing results for query: Llama 3.3 new features and enhancements\n",
      "  Processing result 1: Introducing the new Llama 3.3: Features and Overvi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "  \"main_content\": \"Llama 3.3 is a 70-billion-parameter multilingual large language model that aims to set new standards in efficiency, accessibility, and performance. It builds on the foundation of previous models, introducing several advancements designed to meet the needs of a broader range of users, from small businesses to large enterprises. Llama 3.3 excels in several areas of performance, including better instruction following, improved reasoning, advanced math solving skills, enhanced code generation, and better tool use.\",\n",
      "  \"key_points\": [\n",
      "    \"Improved performance with better contextual understanding and enhanced capabilities across key benchmarks\",\n",
      "    \"Longer context window of up to 128k tokens for handling more complex conversations and summarizing longer documents\",\n",
      "    \"Optimized transformer architecture with Grouped-Query Attention (GQA) for improved scalability and efficiency\",\n",
      "    \"Enhanced multilingual capabilities with improved fluency and understanding across multiple languages\",\n",
      "    \"Cost-effective efficiency with input costs as low as $0.10 per million tokens and output costs at $0.40 per million tokens\"\n",
      "  ],\n",
      "  \"relevance_score\": 9\n",
      "}\n",
      "```\n",
      "  Processing result 2: What is Meta Llama 3.3 70B? Features, Use Cases & ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "  \"main_content\": \"Llama 3.3 is a 70-billion parameter, instruction-tuned AI model optimised for text-based tasks like coding, multilingual tasks, and instruction following. It delivers improved performance compared to Llama 3.1 70B and Llama 3.2 90B in text-based applications. Llama 3.3 features include instruction following, multilingual capabilities, improved code understanding, extended context length, cost-effective performance, synthetic data generation, and more.\",\n",
      "  \"key_points\": [\n",
      "    \"Llama 3.3 excels in interpreting and executing instructions, making it ideal for applications requiring natural language understanding and task completion.\",\n",
      "    \"Llama 3.3 supports multiple languages, ensuring broad usability in diverse linguistic environments, with exceptional performance in tasks requiring multilingual reasoning.\",\n",
      "    \"Llama 3.3 delivers accurate and efficient results for coding tasks, such as code generation and debugging.\",\n",
      "    \"Llama 3.3 offers 405B-level performance at a significantly lower cost, making it an affordable option for developers with budget constraints.\",\n",
      "    \"Llama 3.3 enables efficient synthetic data generation, helping developers address challenges like privacy restrictions and data scarcity.\"\n",
      "  ],\n",
      "  \"relevance_score\": 9\n",
      "}\n",
      "```\n",
      "  Processing result 3: Key Features and Improvements in LLaMA 3.3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "  \"main_content\": \"Key Features and Improvements in LLaMA 3.3: What You Need to Know - AI Resources AIÂ REsources Home Key Features and Improvements in LLaMA 3.3: What You Need to Know Popular ML Compiler Technical Primer Quantization Technical Primer Mixtral of Experts Efficient Memory Management for LLM Serving with PagedAttention RoBERTa: A Robustly Optimized BERT Pretraining Approach + View more Categories Mixture of Experts (MoE) DeepSeek-R1 Test Time Compute AMD MI300X NVIDIA H100 NVIDIA H200 NVIDIA A100 Embedding Models Offline Batch Inference Text Embedding Prometheus & Grafana Speculative Decoding Prefix Caching GGUF Models FP8 with LLMs LLM Serving Function Calling Structured JSON KV Cache AI Foundations Research Industry Agents Context Windows Models ML Systems Key Features and Improvements in LLaMA 3.3: What You Need to Know Next Models LLaMA 3.3 Explained: An Introductory Guide to Meta's Latest AI Model Models Fine-Tuning LLaMA 3.3: A Practical Guide to Customizing the Model for Your Needs On this page Link 1 Link 2 Deploy Gen AI right now Get started View License MAX for Enterprise\",\n",
      "  \"key_points\": [\n",
      "    \"Key Features and Improvements in LLaMA 3.3\",\n",
      "    \"Mixture of Experts (MoE)\",\n",
      "    \"DeepSeek-R1\",\n",
      "    \"Test Time Compute\",\n",
      "    \"Efficient Memory Management for LLM Serving\"\n",
      "  ],\n",
      "  \"relevance_score\": 9\n",
      "}\n",
      "```\n",
      "  Processing result 4: Everything You Need to Know About Llama 3.3 | by A...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "  \"main_content\": \"Members Online • Ok_Ostrich_8845 Llama 3 vs 3.1 vs 3.2 Question | Help What can you say about these 3 versions of Llama LLMs? Were they trained around the same time? Or 3.2 and 3.1 were later enhancement from 3?\",\n",
      "  \"key_points\": [\n",
      "    \"Comparison of Llama 3.3 and Llama 3.1\",\n",
      "    \"Training time and enhancements\",\n",
      "    \"Discussion about Llama LLMs\"\n",
      "  ],\n",
      "  \"relevance_score\": 8\n",
      "}\n",
      "```\n",
      "  Processing result 2: Llama 3 vs Llama 3.1 : Which is Better for Your AI...\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting HTML parsing process with LLM...\")\n",
    "all_parsed_results = process_all_search_results()\n",
    "\n",
    "print(\"\\nGenerating report summaries...\")\n",
    "report_summaries = generate_report_summaries(all_parsed_results)\n",
    "\n",
    "print(\"\\nProcessing complete. Results saved to:\")\n",
    "print(f\"- All parsed results: {parsed_dir / 'all_parsed_results.json'}\")\n",
    "print(f\"- Report summaries: {parsed_dir / 'report_summaries.json'}\")\n",
    "\n",
    "total_queries = len(all_parsed_results)\n",
    "total_results = sum(len(query[\"parsed_results\"]) for query in all_parsed_results)\n",
    "total_reports = len(report_summaries)\n",
    "\n",
    "print(f\"\\nSummary Statistics:\")\n",
    "print(f\"- Total Reports: {total_reports}\")\n",
    "print(f\"- Total Queries: {total_queries}\")\n",
    "print(f\"- Total Results Parsed: {total_results}\")\n",
    "\n",
    "relevance_scores = [\n",
    "    result.get(\"relevance_score\", 0) \n",
    "    for query in all_parsed_results \n",
    "    for result in query[\"parsed_results\"]\n",
    "    if \"relevance_score\" in result\n",
    "]\n",
    "\n",
    "if relevance_scores:\n",
    "    avg_relevance = sum(relevance_scores) / len(relevance_scores)\n",
    "    print(f\"- Average Relevance Score: {avg_relevance:.2f}/10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fca3762-2400-4729-b1e5-4e8c069ecf01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
