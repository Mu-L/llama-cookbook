{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & Env Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from datasets import load_dataset\n",
    "\n",
    "import dspy\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "\n",
    "from benchmarks import llama_mmlu_pro\n",
    "from benchmarks.store_llama_mmlu_pro import store_results, store_optimization_results, store_evaluation_results\n",
    "from benchmarks.statistical_eval import StatisticalEvaluate\n",
    "\n",
    "from config import MODEL_CONFIGS\n",
    "\n",
    "\n",
    "import weave\n",
    "weave.init(project_name=\"mmlu-pro-optimization\")  # You can change the project name as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_THREADS = 36\n",
    "benchmark = llama_mmlu_pro\n",
    "STATISTICAL_EVAL = False\n",
    "FEW_SHOT_BASELINE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_MODEL_NAME = \"vllm_llama_8b\"\n",
    "TASK_MODEL_SETTINGS = MODEL_CONFIGS.get(TASK_MODEL_NAME, MODEL_CONFIGS[TASK_MODEL_NAME])\n",
    "\n",
    "JUDGE_MODEL_NAME = \"openrouter_gpt4o\"\n",
    "JUDGE_MODEL_SETTINGS = MODEL_CONFIGS.get(JUDGE_MODEL_NAME, MODEL_CONFIGS[JUDGE_MODEL_NAME])\n",
    "\n",
    "TASK_MODEL = dspy.LM(\n",
    "    TASK_MODEL_SETTINGS[\"model\"],\n",
    "    api_base=TASK_MODEL_SETTINGS[\"api_base\"],\n",
    "    api_key=TASK_MODEL_SETTINGS[\"api_key\"],\n",
    "    cache=False\n",
    ")\n",
    "\n",
    "JUDGE_MODEL = dspy.LM(\n",
    "    JUDGE_MODEL_SETTINGS[\"model\"],\n",
    "    api_base=JUDGE_MODEL_SETTINGS[\"api_base\"],\n",
    "    api_key=JUDGE_MODEL_SETTINGS[\"api_key\"],\n",
    "    cache=False\n",
    ")\n",
    "\n",
    "dspy.configure(lm=TASK_MODEL)\n",
    "\n",
    "\n",
    "# Create a partial metric function that includes the judge_lm\n",
    "from functools import partial\n",
    "metric_with_judge = partial(benchmark.metric, judge_lm=JUDGE_MODEL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Prompts and Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FEW_SHOT_BASELINE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mFEW_SHOT_BASELINE\u001b[49m:\n\u001b[1;32m      2\u001b[0m     program \u001b[38;5;241m=\u001b[39m dspy\u001b[38;5;241m.\u001b[39mChainOfThought(\n\u001b[1;32m      3\u001b[0m        benchmark\u001b[38;5;241m.\u001b[39msignature(\n\u001b[1;32m      4\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"You are a helpful assistant. \"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     ))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FEW_SHOT_BASELINE' is not defined"
     ]
    }
   ],
   "source": [
    "if FEW_SHOT_BASELINE:\n",
    "    program = dspy.ChainOfThought(\n",
    "       benchmark.signature(\n",
    "        \"\"\"You are a helpful assistant. Few shots examples:\n",
    "        1. \n",
    "        2.\n",
    "        3.\n",
    "        4.\n",
    "        5.\n",
    "        \"\"\"\n",
    "    ))\n",
    "else:\n",
    "    program = dspy.ChainOfThought(\n",
    "   benchmark.signature(\n",
    "    \"\"\"You are a helpful assistant.\"\"\"\n",
    "))\n",
    "\n",
    "\n",
    "evaluate = dspy.Evaluate(\n",
    "    devset=[],\n",
    "    max_errors=500,\n",
    "    metric=metric_with_judge,  # Use the partial function that includes JUDGE_MODEL\n",
    "    num_threads=NUM_THREADS,\n",
    "    display_progress=True,\n",
    "    display_table=True,\n",
    "    return_all_scores=True,\n",
    "    return_outputs=True,\n",
    "    provide_traceback=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, valset, testset = benchmark.datasets(\n",
    "    train_size=0.25,\n",
    "    validation_size=0.25,\n",
    ")\n",
    "\n",
    "len(trainset), len(valset), len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset = trainset + valset + testset\n",
    "len(combined_dataset)\n",
    "TESTSET = combined_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Starting execution...\")\n",
    "\n",
    "if STATISTICAL_EVAL:\n",
    "    evaluate = StatisticalEvaluate(\n",
    "        n_runs=5,  # Number of evaluation runs for statistical significance\n",
    "        confidence_level=0.95,  # 95% confidence interval\n",
    "        devset=[],\n",
    "        max_errors=500,\n",
    "        metric=metric_with_judge,  # Use the partial function that includes JUDGE_MODEL\n",
    "        num_threads=NUM_THREADS,\n",
    "        display_progress=True,\n",
    "        display_table=True,\n",
    "        return_all_scores=True,\n",
    "        return_outputs=True,\n",
    "    )\n",
    "    \n",
    "    # Then modify the evaluation call to:\n",
    "    \n",
    "    stats_results = evaluate(\n",
    "        program,\n",
    "        devset=TESTSET,\n",
    "        statistical=True  # Enable statistical evaluation\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nStatistical Results:\")\n",
    "    print(f\"Mean Score: {stats_results.mean_score:.2f}\")\n",
    "    print(f\"Standard Deviation: {stats_results.std_dev:.2f}\")\n",
    "    print(f\"95% Confidence Interval: ({stats_results.confidence_interval[0]:.2f}, {stats_results.confidence_interval[1]:.2f})\")\n",
    "    print(f\"Number of Runs: {stats_results.n_runs}\")\n",
    "    print(f\"Sample Size: {stats_results.sample_size}\")\n",
    "\n",
    "    run_number = store_results(\n",
    "        task_model=TASK_MODEL.model,\n",
    "        judge_model=JUDGE_MODEL.model,\n",
    "        program=program,  \n",
    "        stats_results=stats_results,  \n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "else:\n",
    "    # eval_subset_size = len(testset)\n",
    "    score, results, all_scores = evaluate(\n",
    "        program,\n",
    "        devset=TESTSET,\n",
    "    )\n",
    "\n",
    "    run_number = store_results(\n",
    "        task_model=TASK_MODEL.model,\n",
    "        judge_model=JUDGE_MODEL.model,\n",
    "        program=program,  \n",
    "        results=results,  \n",
    "        score=score,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Optimization + Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Lite Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEW_SHOTS = 5\n",
    "\n",
    "PROMPT_MODEL_NAME = \"vllm_llama_70b\"\n",
    "PROMPT_MODEL_SETTINGS = MODEL_CONFIGS.get(PROMPT_MODEL_NAME, MODEL_CONFIGS[PROMPT_MODEL_NAME])\n",
    "\n",
    "PROMPT_MODEL = dspy.LM(\n",
    "    PROMPT_MODEL_SETTINGS[\"model\"],\n",
    "    api_base=PROMPT_MODEL_SETTINGS[\"api_base\"],\n",
    "    api_key=PROMPT_MODEL_SETTINGS[\"api_key\"],\n",
    "    cache=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Lite Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "OPTIMIZER=\"light\"\n",
    "\n",
    "optimizer = dspy.MIPROv2(\n",
    "    metric=benchmark.metric,\n",
    "    auto=OPTIMIZER,\n",
    "    num_threads=NUM_THREADS,\n",
    "    task_model=TASK_MODEL,\n",
    "    prompt_model=PROMPT_MODEL,\n",
    "    max_labeled_demos=FEW_SHOTS,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "optimized_program = optimizer.compile(\n",
    "    program,\n",
    "    trainset=trainset,\n",
    "    valset=valset,\n",
    "    requires_permission_to_run=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_number = store_optimization_results(\n",
    "    task_model=TASK_MODEL.model,\n",
    "    judge_model=JUDGE_MODEL.model,\n",
    "    program=optimized_program,\n",
    "    optimization=OPTIMIZER\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"BEST PROMPT:\\n\", optimized_program.signature.instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"BEST EXAMPLES:\\n\", optimized_program.demos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Starting execution...\")\n",
    "STATISTICAL_EVAL=True\n",
    "if STATISTICAL_EVAL:\n",
    "    evaluate = StatisticalEvaluate(\n",
    "        n_runs=5,  # Number of evaluation runs for statistical significance\n",
    "        confidence_level=0.95,  # 95% confidence interval\n",
    "        devset=[],\n",
    "        max_errors=500,\n",
    "        metric=metric_with_judge,  # Use the partial function that includes JUDGE_MODEL\n",
    "        num_threads=NUM_THREADS,\n",
    "        display_progress=True,\n",
    "        display_table=True,\n",
    "        return_all_scores=True,\n",
    "        return_outputs=True,\n",
    "    )\n",
    "    \n",
    "    # Run the evaluation\n",
    "    stats_results = evaluate(\n",
    "        optimized_program,\n",
    "        devset=TESTSET,\n",
    "        statistical=True  # Enable statistical evaluation\n",
    "    )\n",
    "    \n",
    "    # Print statistical results\n",
    "    print(f\"\\nStatistical Results:\")\n",
    "    print(f\"Mean Score: {stats_results.mean_score:.2f}\")\n",
    "    print(f\"Standard Deviation: {stats_results.std_dev:.2f}\")\n",
    "    print(f\"95% Confidence Interval: ({stats_results.confidence_interval[0]:.2f}, {stats_results.confidence_interval[1]:.2f})\")\n",
    "    print(f\"Number of Runs: {stats_results.n_runs}\")\n",
    "    print(f\"Sample Size: {stats_results.sample_size}\")\n",
    "    \n",
    "    # Store evaluation results\n",
    "    store_evaluation_results(\n",
    "        run_number=run_number,  # Use the run_number from optimization\n",
    "        task_model=TASK_MODEL.model,\n",
    "        stats_results=stats_results\n",
    "    )\n",
    "\n",
    "else:\n",
    "    # Run regular evaluation\n",
    "    score, results, all_scores = evaluate(\n",
    "        optimized_program,\n",
    "        devset=TESTSET,\n",
    "    )\n",
    "\n",
    "    # Store evaluation results\n",
    "    store_evaluation_results(\n",
    "        run_number=run_number,  # Use the run_number from optimization\n",
    "        task_model=TASK_MODEL.model,\n",
    "        results=results,\n",
    "        score=score\n",
    "    )\n",
    "\n",
    "print(f\"Completed run_{run_number}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medium Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "subset_size = 500\n",
    "optimizer = dspy.MIPROv2(\n",
    "    metric=benchmark.metric,\n",
    "    auto=\"medium\",\n",
    "    num_threads=NUM_THREADS,\n",
    "    task_model=TASK_MODEL,\n",
    "    prompt_model=PROMPT_MODEL,\n",
    "    max_labeled_demos=FEW_SHOTS,\n",
    ")\n",
    "\n",
    "optimized_program = optimizer.compile(\n",
    "    program,\n",
    "    trainset=trainset[:subset_size],\n",
    "    valset=valset[:subset_size],\n",
    "    requires_permission_to_run=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BEST PROMPT:\\n\", optimized_program.predict.signature.instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BEST EXAMPLES:\\n\", optimized_program.predict.demos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Starting execution...\")\n",
    "\n",
    "if STATISTICAL_EVAL:\n",
    "    evaluate = StatisticalEvaluate(\n",
    "        n_runs=5,  # Number of evaluation runs for statistical significance\n",
    "        confidence_level=0.95,  # 95% confidence interval\n",
    "        devset=[],\n",
    "        max_errors=500,\n",
    "        metric=metric_with_judge,  # Use the partial function that includes JUDGE_MODEL\n",
    "        num_threads=NUM_THREADS,\n",
    "        display_progress=True,\n",
    "        display_table=True,\n",
    "        return_all_scores=True,\n",
    "        return_outputs=True,\n",
    "    )\n",
    "    \n",
    "    # Then modify the evaluation call to:\n",
    "    \n",
    "    stats_results = evaluate(\n",
    "        program,\n",
    "        devset=TESTSET,\n",
    "        statistical=True  # Enable statistical evaluation\n",
    "    )\n",
    "    \n",
    "    # Print statistical results\n",
    "    print(f\"\\nStatistical Results:\")\n",
    "    print(f\"Mean Score: {stats_results.mean_score:.2f}\")\n",
    "    print(f\"Standard Deviation: {stats_results.std_dev:.2f}\")\n",
    "    print(f\"95% Confidence Interval: ({stats_results.confidence_interval[0]:.2f}, {stats_results.confidence_interval[1]:.2f})\")\n",
    "    print(f\"Number of Runs: {stats_results.n_runs}\")\n",
    "    print(f\"Sample Size: {stats_results.sample_size}\")\n",
    "    \n",
    "    # If you want to compare two programs:\n",
    "    # stats_results_a, stats_results_b, p_value = evaluate.compare_programs(\n",
    "    #     program_a=program,\n",
    "    #     program_b=optimized_program,\n",
    "    #     devset=testset\n",
    "    # )\n",
    "    \n",
    "    # print(\"\\nProgram Comparison:\")\n",
    "    # print(f\"Program A Mean Score: {stats_results_a.mean_score:.2f}\")\n",
    "    # print(f\"Program B Mean Score: {stats_results_b.mean_score:.2f}\")\n",
    "    # print(f\"P-value: {p_value:.4f}\")\n",
    "    # print(f\"Statistically Significant: {p_value < 0.05}\")\n",
    "\n",
    "    run_number = store_results(\n",
    "        task_model=TASK_MODEL.model,\n",
    "        judge_model=JUDGE_MODEL.model,\n",
    "        program=optimized_program,  \n",
    "        stats_results=stats_results,  \n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "else:\n",
    "    # eval_subset_size = len(testset)\n",
    "    score, results, all_scores = evaluate(\n",
    "        program,\n",
    "        devset=TESTSET,\n",
    "    )\n",
    "\n",
    "    run_number = store_results(\n",
    "        task_model=TASK_MODEL.model,\n",
    "        judge_model=JUDGE_MODEL.model,\n",
    "        program=optimized_program,  \n",
    "        results=results,  \n",
    "        score=score,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heavy Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "NUM_THREADS = 48\n",
    "OPTIMIZER = 'heavy'\n",
    "\n",
    "optimizer = dspy.MIPROv2(\n",
    "    metric=benchmark.metric,\n",
    "    auto=OPTIMIZER,\n",
    "    num_threads=NUM_THREADS,\n",
    "    task_model=TASK_MODEL,\n",
    "    prompt_model=PROMPT_MODEL,\n",
    "    max_labeled_demos=FEW_SHOTS,\n",
    ")\n",
    "\n",
    "optimized_program = optimizer.compile(\n",
    "    program,\n",
    "    trainset=trainset,\n",
    "    valset=valset,\n",
    "    requires_permission_to_run=False,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_number = store_optimization_results(\n",
    "    task_model=TASK_MODEL.model,\n",
    "    judge_model=JUDGE_MODEL.model,\n",
    "    program=optimized_program,\n",
    "    optimization=OPTIMIZER\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BEST PROMPT:\\n\", optimized_program.signature.instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"BEST EXAMPLES:\\n\", optimized_program.demos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Starting execution...\")\n",
    "\n",
    "if STATISTICAL_EVAL:\n",
    "    evaluate = StatisticalEvaluate(\n",
    "        n_runs=5,  # Number of evaluation runs for statistical significance\n",
    "        confidence_level=0.95,  # 95% confidence interval\n",
    "        devset=[],\n",
    "        max_errors=500,\n",
    "        metric=metric_with_judge,  # Use the partial function that includes JUDGE_MODEL\n",
    "        num_threads=NUM_THREADS,\n",
    "        display_progress=True,\n",
    "        display_table=True,\n",
    "        return_all_scores=True,\n",
    "        return_outputs=True,\n",
    "    )\n",
    "    \n",
    "    # Run the evaluation\n",
    "    stats_results = evaluate(\n",
    "        optimized_program,\n",
    "        devset=TESTSET,\n",
    "        statistical=True  # Enable statistical evaluation\n",
    "    )\n",
    "    \n",
    "    # Print statistical results\n",
    "    print(f\"\\nStatistical Results:\")\n",
    "    print(f\"Mean Score: {stats_results.mean_score:.2f}\")\n",
    "    print(f\"Standard Deviation: {stats_results.std_dev:.2f}\")\n",
    "    print(f\"95% Confidence Interval: ({stats_results.confidence_interval[0]:.2f}, {stats_results.confidence_interval[1]:.2f})\")\n",
    "    print(f\"Number of Runs: {stats_results.n_runs}\")\n",
    "    print(f\"Sample Size: {stats_results.sample_size}\")\n",
    "    \n",
    "    # Store evaluation results\n",
    "    store_evaluation_results(\n",
    "        run_number=run_number,  # Use the run_number from optimization\n",
    "        task_model=TASK_MODEL.model,\n",
    "        stats_results=stats_results\n",
    "    )\n",
    "\n",
    "else:\n",
    "    # Run regular evaluation\n",
    "    score, results, all_scores = evaluate(\n",
    "        optimized_program,\n",
    "        devset=TESTSET,\n",
    "    )\n",
    "\n",
    "    # Store evaluation results\n",
    "    store_evaluation_results(\n",
    "        run_number=run_number,  # Use the run_number from optimization\n",
    "        task_model=TASK_MODEL.model,\n",
    "        results=results,\n",
    "        score=score\n",
    "    )\n",
    "\n",
    "print(f\"Completed run_{run_number}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
